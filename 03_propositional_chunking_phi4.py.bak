"""
03_propositional_chunking.py
prepared_contentsì˜ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ Phi-4 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬
propositional chunkingìœ¼ë¡œ ë¶„í• í•˜ê³  parquet íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ëª¨ë“ˆ
"""

import re
import json
import yaml
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Any, Optional

import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import onnxruntime_genai as og  # type: ignore[import-untyped]


# ë””ë ‰í„°ë¦¬ ì„¤ì •
BASE_DIR = Path(__file__).parent
INPUT_DIR = BASE_DIR / "prepared_contents"
OUTPUT_DIR = BASE_DIR / "chunked_data"
MODEL_DIR = BASE_DIR / "models" / "phi-4-onnx"


# Propositional Chunking í”„ë¡¬í”„íŠ¸
CHUNKING_PROMPT_TEMPLATE = """You are a document chunking expert. Your task is to split the following document into meaningful, self-contained propositions (chunks).

Each chunk should:
1. Be a complete, standalone piece of information
2. Contain a single main idea or concept
3. Be understandable without needing other chunks
4. Preserve important context when necessary

Document:
---
{content}
---

Split this document into logical chunks. Return a JSON array where each element is a chunk object with:
- "chunk_text": The actual text content of the chunk
- "chunk_type": Type of content ("header", "paragraph", "list", "definition", "example", "conclusion")
- "topic": Brief topic description (2-5 words)

Return ONLY valid JSON array, no additional text. Example format:
[
  {{"chunk_text": "...", "chunk_type": "header", "topic": "Introduction"}},
  {{"chunk_text": "...", "chunk_type": "paragraph", "topic": "Main concept"}}
]"""


class PropositionalChunker:
    """Phi-4 ONNX ëª¨ë¸ì„ ì‚¬ìš©í•œ Propositional Chunking ì²˜ë¦¬ê¸°"""
    
    def __init__(self, model_dir: Path = MODEL_DIR):
        """
        Args:
            model_dir: ONNX ëª¨ë¸ì´ ì €ì¥ëœ ë””ë ‰í„°ë¦¬
        """
        self.model_dir = Path(model_dir)
        self.model: Any = None
        self.tokenizer: Any = None
        self._load_model()
    
    def _load_model(self) -> None:
        """ONNX ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_dir}")
        
        if not self.model_dir.exists():
            raise FileNotFoundError(
                f"ëª¨ë¸ ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {self.model_dir}\n"
                "ë¨¼ì € 01_download_model.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”."
            )
        
        try:
            Model = getattr(og, "Model")
            Tokenizer = getattr(og, "Tokenizer")
            self.model = Model(str(self.model_dir))
            self.tokenizer = Tokenizer(self.model)
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            raise RuntimeError(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    
    def generate_response(
        self,
        prompt: str,
        max_length: int = 4096,
        temperature: float = 0.3,
        top_p: float = 0.9,
    ) -> str:
        """
        Phi-4 ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„±
        """
        # Phi-4 chat í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        chat_prompt = f"<|user|>\n{prompt}<|end|>\n<|assistant|>\n"
        
        # í† í°í™”
        input_tokens = self.tokenizer.encode(chat_prompt)
        
        # ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
        GeneratorParams = getattr(og, "GeneratorParams")
        Generator = getattr(og, "Generator")
        params = GeneratorParams(self.model)
        params.set_search_options(
            max_length=max_length,
            temperature=temperature,
            top_p=top_p,
        )
        
        # í…ìŠ¤íŠ¸ ìƒì„±
        generator = Generator(self.model, params)
        generator.append_tokens(input_tokens)
        
        output_tokens = []
        
        while not generator.is_done():
            generator.generate_next_token()
            new_token = generator.get_next_tokens()[0]
            output_tokens.append(new_token)
        
        # ë””ì½”ë”©
        response = self.tokenizer.decode(np.array(output_tokens, dtype=np.int32))
        
        # ì¢…ë£Œ í† í° ì œê±°
        if "<|end|>" in response:
            response = response.split("<|end|>")[0]
        
        return response.strip()
    
    def chunk_document(self, content: str, metadata: dict) -> list[dict]:
        """
        ë¬¸ì„œë¥¼ propositional chunksë¡œ ë¶„í• 
        
        Args:
            content: ë¬¸ì„œ ë‚´ìš© (front matter ì œì™¸)
            metadata: ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
            
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        # ë„ˆë¬´ ê¸´ ë¬¸ì„œëŠ” ë¶„í•  ì²˜ë¦¬
        max_content_length = 6000
        if len(content) > max_content_length:
            return self._chunk_long_document(content, metadata)
        
        prompt = CHUNKING_PROMPT_TEMPLATE.format(content=content)
        
        try:
            response = self.generate_response(prompt, temperature=0.2)
            
            # JSON íŒŒì‹±
            json_start = response.find("[")
            json_end = response.rfind("]") + 1
            
            if json_start != -1 and json_end > json_start:
                json_str = response[json_start:json_end]
                chunks = json.loads(json_str)
                
                # ìœ íš¨ì„± ê²€ì¦ ë° ì •ê·œí™”
                validated_chunks = []
                for i, chunk in enumerate(chunks):
                    if isinstance(chunk, dict) and "chunk_text" in chunk:
                        validated_chunks.append({
                            "chunk_text": chunk.get("chunk_text", ""),
                            "chunk_type": chunk.get("chunk_type", "paragraph"),
                            "topic": chunk.get("topic", ""),
                        })
                
                if validated_chunks:
                    return validated_chunks
            
            print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨, í´ë°± ì²­í‚¹ ì ìš©")
            return self._fallback_chunking(content)
                
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜: {e}, í´ë°± ì²­í‚¹ ì ìš©")
            return self._fallback_chunking(content)
        except Exception as e:
            print(f"âš ï¸ ì²­í‚¹ ì¤‘ ì˜¤ë¥˜: {e}, í´ë°± ì²­í‚¹ ì ìš©")
            return self._fallback_chunking(content)
    
    def _chunk_long_document(self, content: str, metadata: dict) -> list[dict]:
        """ê¸´ ë¬¸ì„œë¥¼ ì„¹ì…˜ë³„ë¡œ ë‚˜ëˆ ì„œ ì²­í‚¹"""
        # ë§ˆí¬ë‹¤ìš´ í—¤ë” ê¸°ì¤€ìœ¼ë¡œ ì„¹ì…˜ ë¶„í• 
        sections = re.split(r'\n(?=#{1,3}\s)', content)
        
        all_chunks = []
        for section in sections:
            section = section.strip()
            if not section:
                continue
            
            if len(section) > 6000:
                # ì„¹ì…˜ì´ ì—¬ì „íˆ ê¸¸ë©´ í´ë°± ì²­í‚¹
                all_chunks.extend(self._fallback_chunking(section))
            else:
                chunks = self.chunk_document(section, metadata)
                all_chunks.extend(chunks)
        
        return all_chunks
    
    def _fallback_chunking(self, content: str) -> list[dict]:
        """LLM ì²­í‚¹ ì‹¤íŒ¨ ì‹œ ê·œì¹™ ê¸°ë°˜ í´ë°± ì²­í‚¹"""
        chunks = []
        
        # ë§ˆí¬ë‹¤ìš´ í—¤ë”ì™€ ë‹¨ë½ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
        lines = content.split('\n')
        current_chunk = []
        current_type = "paragraph"
        
        for line in lines:
            stripped = line.strip()
            
            # í—¤ë” ê°ì§€
            if stripped.startswith('#'):
                # ì´ì „ ì²­í¬ ì €ì¥
                if current_chunk:
                    chunk_text = '\n'.join(current_chunk).strip()
                    if chunk_text:
                        chunks.append({
                            "chunk_text": chunk_text,
                            "chunk_type": current_type,
                            "topic": "",
                        })
                    current_chunk = []
                
                current_chunk.append(line)
                current_type = "header"
            
            # ë¹ˆ ì¤„ - ë‹¨ë½ êµ¬ë¶„
            elif not stripped:
                if current_chunk:
                    chunk_text = '\n'.join(current_chunk).strip()
                    if chunk_text:
                        chunks.append({
                            "chunk_text": chunk_text,
                            "chunk_type": current_type,
                            "topic": "",
                        })
                    current_chunk = []
                    current_type = "paragraph"
            
            # ë¦¬ìŠ¤íŠ¸ í•­ëª©
            elif stripped.startswith(('-', '*', '1.', '2.', '3.')):
                current_chunk.append(line)
                current_type = "list"
            
            else:
                current_chunk.append(line)
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk:
            chunk_text = '\n'.join(current_chunk).strip()
            if chunk_text:
                chunks.append({
                    "chunk_text": chunk_text,
                    "chunk_type": current_type,
                    "topic": "",
                })
        
        return chunks


def parse_markdown_with_frontmatter(file_path: Path) -> tuple[dict, str]:
    """
    YAML front matterê°€ ìˆëŠ” ë§ˆí¬ë‹¤ìš´ íŒŒì¼ íŒŒì‹±
    
    Returns:
        (metadata, content) íŠœí”Œ
    """
    text = file_path.read_text(encoding="utf-8")
    
    # YAML front matter ì¶”ì¶œ
    if text.startswith("---"):
        parts = text.split("---", 2)
        if len(parts) >= 3:
            try:
                metadata = yaml.safe_load(parts[1])
                content = parts[2].strip()
                return metadata or {}, content
            except yaml.YAMLError:
                pass
    
    return {}, text


def generate_chunk_id(source_file: str, chunk_index: int, chunk_text: str) -> str:
    """ì²­í¬ ê³ ìœ  ID ìƒì„±"""
    hash_input = f"{source_file}:{chunk_index}:{chunk_text[:100]}"
    return hashlib.md5(hash_input.encode()).hexdigest()[:16]


def generate_content_hash(text: str) -> str:
    """ì½˜í…ì¸  í•´ì‹œ ìƒì„± (ë³€ê²½ ê°ì§€ìš©)"""
    return hashlib.sha256(text.encode()).hexdigest()


def generate_source_hash(content: str, metadata: dict) -> str:
    """ì†ŒìŠ¤ íŒŒì¼ ì „ì²´ í•´ì‹œ (ë¬¸ì„œ ë³€ê²½ ê°ì§€ìš©)"""
    hash_input = content + json.dumps(metadata, sort_keys=True, ensure_ascii=False)
    return hashlib.sha256(hash_input.encode()).hexdigest()[:32]


def load_existing_parquet(file_path: Path) -> tuple[pd.DataFrame | None, dict]:
    """
    ê¸°ì¡´ parquet íŒŒì¼ ë¡œë“œ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    
    Returns:
        (DataFrame, metadata) íŠœí”Œ. íŒŒì¼ì´ ì—†ìœ¼ë©´ (None, {})
    """
    if not file_path.exists():
        return None, {}
    
    try:
        table = pq.read_table(file_path)
        df = table.to_pandas()
        
        # íŒŒì¼ ë©”íƒ€ë°ì´í„°ì—ì„œ ë²„ì „ ì •ë³´ ì¶”ì¶œ
        file_meta = table.schema.metadata or {}
        metadata = {
            "version": int(file_meta.get(b"version", b"0")),
            "source_hash": file_meta.get(b"source_hash", b"").decode(),
            "created_at": file_meta.get(b"created_at", b"").decode(),
            "updated_at": file_meta.get(b"updated_at", b"").decode(),
        }
        return df, metadata
    except Exception as e:
        print(f"âš ï¸ ê¸°ì¡´ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, {}


def check_needs_update(existing_meta: dict, new_source_hash: str) -> bool:
    """
    ì†ŒìŠ¤ í•´ì‹œ ë¹„êµë¡œ ì—…ë°ì´íŠ¸ í•„ìš” ì—¬ë¶€ í™•ì¸
    """
    if not existing_meta:
        return True
    return existing_meta.get("source_hash") != new_source_hash


def merge_chunks(
    existing_df: pd.DataFrame | None,
    new_records: list[dict],
    existing_meta: dict,
) -> tuple[list[dict], dict]:
    """
    ê¸°ì¡´ ì²­í¬ì™€ ìƒˆ ì²­í¬ ë³‘í•© (ì¦ë¶„ ì—…ë°ì´íŠ¸)
    
    ìƒˆë¡œìš´ ì²­í‚¹ ê²°ê³¼ì— ì—†ëŠ” ê¸°ì¡´ ì²­í¬ë“¤ì€ ìë™ìœ¼ë¡œ ì œê±°ë©ë‹ˆë‹¤.
    
    Returns:
        (merged_records, update_stats) íŠœí”Œ
    """
    stats = {"added": 0, "updated": 0, "unchanged": 0, "deleted": 0}
    
    if existing_df is None or existing_df.empty:
        stats["added"] = len(new_records)
        return new_records, stats
    
    # ê¸°ì¡´ ì²­í¬ ì¸ë±ì‹±
    existing_hashes = set(existing_df["content_hash"].tolist()) if "content_hash" in existing_df.columns else set()
    existing_chunk_ids = set(existing_df["chunk_id"].tolist())
    
    merged = []
    new_hashes = set()
    new_chunk_ids = set()
    
    for record in new_records:
        content_hash = record["content_hash"]
        chunk_id = record["chunk_id"]
        new_hashes.add(content_hash)
        new_chunk_ids.add(chunk_id)
        
        if content_hash in existing_hashes:
            # ë™ì¼í•œ ì½˜í…ì¸  - ê¸°ì¡´ ë²„ì „ ìœ ì§€
            existing_row = existing_df[existing_df["content_hash"] == content_hash].iloc[0]
            record["version"] = int(existing_row["version"])
            record["created_at"] = existing_row["created_at"]
            stats["unchanged"] += 1
        else:
            # ìƒˆë¡œìš´ ë˜ëŠ” ë³€ê²½ëœ ì½˜í…ì¸ 
            # chunk_idê°€ ê°™ì€ ê²Œ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì¶”ê°€
            if chunk_id in existing_chunk_ids:
                existing_row = existing_df[existing_df["chunk_id"] == chunk_id].iloc[0]
                record["version"] = int(existing_row["version"]) + 1
                record["created_at"] = existing_row["created_at"]
                stats["updated"] += 1
            else:
                stats["added"] += 1
        
        merged.append(record)
    
    # ì‚­ì œëœ ì²­í¬ ì¹´ìš´íŠ¸ (ê¸°ì¡´ì— ìˆì—ˆì§€ë§Œ ìƒˆ ì²­í‚¹ì— ì—†ëŠ” ê²ƒë“¤)
    # chunk_id ê¸°ì¤€ìœ¼ë¡œ ì‚­ì œ ê°ì§€ (ë” ì •í™•í•œ ì‚­ì œ ì¶”ì )
    deleted_chunk_ids = existing_chunk_ids - new_chunk_ids
    stats["deleted"] = len(deleted_chunk_ids)
    
    return merged, stats


def process_documents(
    input_dir: Path = INPUT_DIR,
    output_dir: Path = OUTPUT_DIR,
    model_dir: Path = MODEL_DIR,
):
    """
    prepared_contentsì˜ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë“¤ì„ ì²­í‚¹í•˜ì—¬ parquetìœ¼ë¡œ ì €ì¥
    """
    input_dir = Path(input_dir)
    output_dir = Path(output_dir)
    
    # ë””ë ‰í„°ë¦¬ í™•ì¸/ìƒì„±
    if not input_dir.exists():
        print(f"âš ï¸ ì…ë ¥ ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {input_dir}")
        return
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ëª©ë¡
    md_files = list(input_dir.glob("*.md"))
    
    if not md_files:
        print(f"âš ï¸ ì²˜ë¦¬í•  ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {input_dir}")
        return
    
    print(f"\nğŸ“š ì²˜ë¦¬í•  ë¬¸ì„œ: {len(md_files)}ê°œ")
    print("=" * 50)
    
    # ëª¨ë¸ ë¡œë“œ
    chunker = PropositionalChunker(model_dir)
    
    # ê° ë¬¸ì„œ ì²˜ë¦¬
    success_count = 0
    total_chunks = 0
    
    for i, md_file in enumerate(md_files, 1):
        print(f"\n[{i}/{len(md_files)}] ì²˜ë¦¬ ì¤‘: {md_file.name}")
        
        try:
            # ë§ˆí¬ë‹¤ìš´ íŒŒì‹±
            metadata, content = parse_markdown_with_frontmatter(md_file)
            print(f"   ğŸ“– ë¬¸ì„œ ê¸¸ì´: {len(content)} ë¬¸ì")
            
            # Propositional Chunking
            print("   ğŸ” Propositional Chunking ì¤‘...")
            chunks = chunker.chunk_document(content, metadata)
            print(f"   âœ“ ìƒì„±ëœ ì²­í¬: {len(chunks)}ê°œ")
            
            # ì†ŒìŠ¤ í•´ì‹œ ê³„ì‚°
            source_hash = generate_source_hash(content, metadata)
            output_file = output_dir / f"{md_file.stem}.parquet"
            
            # ê¸°ì¡´ íŒŒì¼ í™•ì¸
            existing_df, existing_meta = load_existing_parquet(output_file)
            
            # ë³€ê²½ ì—¬ë¶€ í™•ì¸
            if not check_needs_update(existing_meta, source_hash):
                print(f"   â­ï¸ ë³€ê²½ ì—†ìŒ, ìŠ¤í‚µ")
                success_count += 1
                if existing_df is not None:
                    total_chunks += len(existing_df)
                continue
            
            now = datetime.now().isoformat()
            new_version = existing_meta.get("version", 0) + 1
            created_at = existing_meta.get("created_at") or now
            
            # ë°ì´í„°í”„ë ˆì„ êµ¬ì„±
            records = []
            for idx, chunk in enumerate(chunks):
                chunk_id = generate_chunk_id(md_file.name, idx, chunk["chunk_text"])
                content_hash = generate_content_hash(chunk["chunk_text"])
                
                records.append({
                    "chunk_id": chunk_id,
                    "content_hash": content_hash,
                    "source_file": md_file.name,
                    "chunk_index": idx,
                    "chunk_text": chunk["chunk_text"],
                    "chunk_type": chunk["chunk_type"],
                    "topic": chunk["topic"],
                    # ë©”íƒ€ë°ì´í„° í¬í•¨
                    "domain": metadata.get("domain", ""),
                    "sub_domain": metadata.get("sub_domain", ""),
                    "keywords": json.dumps(metadata.get("keywords", []), ensure_ascii=False),
                    "language": metadata.get("language", ""),
                    "content_type": metadata.get("content_type", ""),
                    # ë²„ì „ ê´€ë¦¬ í•„ë“œ
                    "version": 1,
                    "created_at": now,
                    "updated_at": now,
                })
            
            # ì¦ë¶„ ì—…ë°ì´íŠ¸ ë³‘í•©
            merged_records, update_stats = merge_chunks(existing_df, records, existing_meta)
            
            # ì—…ë°ì´íŠ¸ í†µê³„ ì¶œë ¥
            if existing_df is not None:
                print(f"   ğŸ“Š ì¦ë¶„ ì—…ë°ì´íŠ¸: ì¶”ê°€ {update_stats['added']}, ìˆ˜ì • {update_stats['updated']}, "
                      f"ìœ ì§€ {update_stats['unchanged']}, ì‚­ì œ {update_stats['deleted']}")
            
            # Parquet ì €ì¥
            df = pd.DataFrame(merged_records)
            
            # PyArrow ìŠ¤í‚¤ë§ˆ ì •ì˜
            schema = pa.schema([
                ("chunk_id", pa.string()),
                ("content_hash", pa.string()),
                ("source_file", pa.string()),
                ("chunk_index", pa.int32()),
                ("chunk_text", pa.string()),
                ("chunk_type", pa.string()),
                ("topic", pa.string()),
                ("domain", pa.string()),
                ("sub_domain", pa.string()),
                ("keywords", pa.string()),  # JSON string
                ("language", pa.string()),
                ("content_type", pa.string()),
                ("version", pa.int32()),
                ("created_at", pa.string()),
                ("updated_at", pa.string()),
            ])
            
            # íŒŒì¼ ë©”íƒ€ë°ì´í„° (ë²„ì „ ê´€ë¦¬ìš©)
            file_metadata = {
                b"version": str(new_version).encode(),
                b"source_hash": source_hash.encode(),
                b"created_at": created_at.encode(),
                b"updated_at": now.encode(),
                b"schema_version": b"1.0",
            }
            
            table = pa.Table.from_pandas(df, schema=schema)
            table = table.replace_schema_metadata(file_metadata)
            
            # zstd ì••ì¶•ìœ¼ë¡œ ì €ì¥
            pq.write_table(
                table,
                output_file,
                compression="zstd",
                compression_level=3,  # 1-22, ê¸°ë³¸ê°’ 3 (ì†ë„ì™€ ì••ì¶•ë¥  ê· í˜•)
            )
            
            print(f"   ğŸ’¾ ì €ì¥: {output_file.name} (v{new_version}, zstd ì••ì¶•)")
            
            success_count += 1
            total_chunks += len(chunks)
            
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "=" * 50)
    print(f"âœ… ì™„ë£Œ: {success_count}/{len(md_files)} ë¬¸ì„œ ì²˜ë¦¬ë¨")
    print(f"ğŸ“Š ì´ ì²­í¬ ìˆ˜: {total_chunks}ê°œ")
    print(f"ğŸ“ ì¶œë ¥ ìœ„ì¹˜: {output_dir}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œë¥¼ propositional chunkingí•˜ì—¬ parquetìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."
    )
    parser.add_argument(
        "--input-dir",
        type=Path,
        default=INPUT_DIR,
        help=f"ì…ë ¥ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸ê°’: {INPUT_DIR})",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=OUTPUT_DIR,
        help=f"ì¶œë ¥ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸ê°’: {OUTPUT_DIR})",
    )
    parser.add_argument(
        "--model-dir",
        type=Path,
        default=MODEL_DIR,
        help=f"ëª¨ë¸ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸ê°’: {MODEL_DIR})",
    )
    
    args = parser.parse_args()
    
    process_documents(
        input_dir=args.input_dir,
        output_dir=args.output_dir,
        model_dir=args.model_dir,
    )


if __name__ == "__main__":
    main()
