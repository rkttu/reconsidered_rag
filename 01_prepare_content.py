"""
01_prepare_content.py
Module for extracting metadata from input documents and adding YAML front matter

Features:
- Support for various document formats using Microsoft markitdown
  (Word, Excel, PowerPoint, PDF, HTML, images, audio, etc.)
- Azure AI service integration (Document Intelligence, OpenAI Vision, Speech)
- Metadata extraction through markdown structure analysis
- Automatic keyword extraction (headings, bold, link texts, etc.)
- Language detection support
- YAML front matter generation

Note: LLM enrichment is handled separately by 02_enrich_content.py
"""

import os
import re
from pathlib import Path
from datetime import datetime
from typing import Optional, Any

import pymupdf4llm
import yaml
from dotenv import load_dotenv
from langdetect import detect, LangDetectException
from markitdown import MarkItDown, UnsupportedFormatException


# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ë””ë ‰í„°ë¦¬ ì„¤ì •
BASE_DIR = Path(__file__).parent
INPUT_DIR = BASE_DIR / "input_docs"
OUTPUT_DIR = BASE_DIR / "prepared_contents"

# PDF ì²˜ë¦¬ ë°©ì‹ ì„¤ì •
# "pymupdf4llm" (ê¸°ë³¸ê°’): pymupdf4llm ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© (LLM ìµœì í™”, í‘œ/êµ¬ì¡° ë³´ì¡´)
# "markitdown": Microsoft markitdown ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© (Azure AI ì—°ë™ ê°€ëŠ¥)
PDF_PROCESSOR = os.getenv("PDF_PROCESSOR", "pymupdf4llm")


# =============================================================================
# Azure ì„œë¹„ìŠ¤ ì„¤ì •
# =============================================================================

def get_azure_document_intelligence_client() -> Optional[Any]:
    """Create Azure Document Intelligence client"""
    endpoint = os.getenv("AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT")
    key = os.getenv("AZURE_DOCUMENT_INTELLIGENCE_KEY")
    auth_method = os.getenv("AZURE_AUTH_METHOD", "key")

    if not endpoint:
        return None

    try:
        from azure.ai.documentintelligence import DocumentIntelligenceClient

        if auth_method == "default":
            from azure.identity import DefaultAzureCredential
            credential = DefaultAzureCredential()
        else:
            from azure.core.credentials import AzureKeyCredential
            if not key:
                print("âš ï¸ AZURE_DOCUMENT_INTELLIGENCE_KEY is not set.")
                return None
            credential = AzureKeyCredential(key)

        return DocumentIntelligenceClient(
            endpoint=endpoint,
            credential=credential,
        )
    except ImportError:
        print("âš ï¸ azure-ai-documentintelligence package is not installed.")
        return None
    except Exception as e:
        print(f"âš ï¸ Failed to create Document Intelligence client: {e}")
        return None


def get_azure_openai_client() -> Optional[Any]:
    """Create Azure OpenAI client (GPT-4o Vision)"""
    endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
    key = os.getenv("AZURE_OPENAI_API_KEY")

    if not endpoint or not key:
        return None

    try:
        from openai import AzureOpenAI

        return AzureOpenAI(
            azure_endpoint=endpoint,
            api_key=key,
            api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview"),
        )
    except ImportError:
        print("âš ï¸ openai package is not installed.")
        return None
    except Exception as e:
        print(f"âš ï¸ Failed to create Azure OpenAI client: {e}")
        return None


def get_azure_speech_config() -> Optional[tuple[str, str]]:
    """Azure Speech ì„¤ì • ë°˜í™˜"""
    key = os.getenv("AZURE_SPEECH_KEY")
    region = os.getenv("AZURE_SPEECH_REGION")

    if key and region:
        return key, region
    return None


# =============================================================================
# MarkItDown ì„¤ì •
# =============================================================================

# MarkItDown ì§€ì› íŒŒì¼ í™•ì¥ì
SUPPORTED_EXTENSIONS = {
    # ë¬¸ì„œ
    ".pdf", ".docx", ".doc", ".pptx", ".ppt", ".xlsx", ".xls",
    # ì›¹/í…ìŠ¤íŠ¸
    ".html", ".htm", ".xml", ".json", ".csv",
    # ë§ˆí¬ë‹¤ìš´/í…ìŠ¤íŠ¸
    ".md", ".markdown", ".txt", ".rst",
    # ì´ë¯¸ì§€ (EXIF/OCR)
    ".jpg", ".jpeg", ".png", ".gif", ".bmp", ".webp", ".tiff",
    # ì˜¤ë””ì˜¤ (ìŒì„± ì¸ì‹)
    ".mp3", ".wav", ".m4a", ".ogg", ".flac",
    # ë¹„ë””ì˜¤ (ìë§‰ ì¶”ì¶œ)
    ".mp4", ".mkv", ".avi", ".mov", ".webm",
    # ì½”ë“œ/ê¸°íƒ€
    ".py", ".js", ".ts", ".java", ".c", ".cpp", ".cs", ".go", ".rs",
    ".ipynb",  # Jupyter Notebook
    ".zip",  # Archive (ë‚´ë¶€ íŒŒì¼ ì²˜ë¦¬)
}

# MarkItDown ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_markitdown_instance: Optional[MarkItDown] = None


def get_markitdown() -> MarkItDown:
    """
    MarkItDown ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜

    Azure ì„œë¹„ìŠ¤ê°€ ì„¤ì •ëœ ê²½ìš° ìë™ìœ¼ë¡œ ì—°ë™í•©ë‹ˆë‹¤:
    - Document Intelligence: ìŠ¤ìº” PDF, ì´ë¯¸ì§€ OCR í–¥ìƒ
    - OpenAI (GPT-4o): ì´ë¯¸ì§€ ë‚´ìš© ì´í•´
    """
    global _markitdown_instance
    if _markitdown_instance is None:
        # Azure í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        doc_client = get_azure_document_intelligence_client()
        llm_client = get_azure_openai_client()
        llm_model = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")

        # ì—°ë™ ìƒíƒœ ì¶œë ¥
        services = []
        if doc_client:
            services.append("Document Intelligence")
        if llm_client and llm_model:
            services.append(f"OpenAI ({llm_model})")

        if services:
            print(f"ğŸ”— Azure ì„œë¹„ìŠ¤ ì—°ë™: {', '.join(services)}")
        else:
            print("â„¹ï¸ Azure ì„œë¹„ìŠ¤ ë¯¸ì—°ë™ (ê¸°ë³¸ markitdown ì‚¬ìš©)")

        # MarkItDown ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        _markitdown_instance = MarkItDown(
            document_intelligence_client=doc_client,
            llm_client=llm_client,
            llm_model=llm_model,
        )

    return _markitdown_instance


def is_supported_file(file_path: Path) -> bool:
    """íŒŒì¼ì´ ì§€ì›ë˜ëŠ” í˜•ì‹ì¸ì§€ í™•ì¸"""
    return file_path.suffix.lower() in SUPPORTED_EXTENSIONS


# =============================================================================
# ìœ ë‹ˆì½”ë“œ ìƒìˆ˜ ì •ì˜ (ì¸ì½”ë”© ì•ˆì „)
# =============================================================================

# ì›ë¬¸ì (Circled Numbers) - â‘  ~ â‘³
CIRCLED_NUMBERS = [
    '\u2460',  # â‘  CIRCLED DIGIT ONE
    '\u2461',  # â‘¡ CIRCLED DIGIT TWO
    '\u2462',  # â‘¢ CIRCLED DIGIT THREE
    '\u2463',  # â‘£ CIRCLED DIGIT FOUR
    '\u2464',  # â‘¤ CIRCLED DIGIT FIVE
    '\u2465',  # â‘¥ CIRCLED DIGIT SIX
    '\u2466',  # â‘¦ CIRCLED DIGIT SEVEN
    '\u2467',  # â‘§ CIRCLED DIGIT EIGHT
    '\u2468',  # â‘¨ CIRCLED DIGIT NINE
    '\u2469',  # â‘© CIRCLED DIGIT TEN
    '\u246A',  # â‘ª CIRCLED NUMBER ELEVEN
    '\u246B',  # â‘« CIRCLED NUMBER TWELVE
    '\u246C',  # â‘¬ CIRCLED NUMBER THIRTEEN
    '\u246D',  # â‘­ CIRCLED NUMBER FOURTEEN
    '\u246E',  # â‘® CIRCLED NUMBER FIFTEEN
    '\u246F',  # â‘¯ CIRCLED NUMBER SIXTEEN
    '\u2470',  # â‘° CIRCLED NUMBER SEVENTEEN
    '\u2471',  # â‘± CIRCLED NUMBER EIGHTEEN
    '\u2472',  # â‘² CIRCLED NUMBER NINETEEN
    '\u2473',  # â‘³ CIRCLED NUMBER TWENTY
]

# í•œêµ­ì–´ ì¡°í•­ ì œëª©ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê´„í˜¸
LEFT_BLACK_LENTICULAR_BRACKET = '\u3010'  # ã€
RIGHT_BLACK_LENTICULAR_BRACKET = '\u3011'  # ã€‘


def normalize_line_breaks(text: str) -> str:
    """
    PDFì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì˜ ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆì„ ì •ë¦¬í•©ë‹ˆë‹¤.

    PDFëŠ” í˜ì´ì§€ ë ˆì´ì•„ì›ƒì— ë§ì¶° í…ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆí•˜ê¸° ë•Œë¬¸ì—,
    ë¬¸ì¥ ì¤‘ê°„ì— ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ëŠ” ë¬¸ì¥ ì¤‘ê°„ì˜ ì¤„ë°”ê¿ˆì„ ì œê±°í•˜ê³  ë¬¸ë‹¨ êµ¬ë¶„ì€ ìœ ì§€í•©ë‹ˆë‹¤.

    Args:
        text: ì •ë¦¬í•  í…ìŠ¤íŠ¸

    Returns:
        ì¤„ë°”ê¿ˆì´ ì •ë¦¬ëœ í…ìŠ¤íŠ¸
    """
    lines = text.split('\n')
    result_lines = []
    current_paragraph = []
    pending_empty_lines = 0  # ì—°ì† ë¹ˆ ì¤„ ì¹´ìš´íŠ¸

    # ìƒˆ ë¸”ë¡ ì‹œì‘ íŒ¨í„´ (ì¡°í•­ ì œëª© ë“±)
    # "ì œ1 ì¡° ã€..." ë˜ëŠ” "ì œ1ì¡°ã€..." í˜•íƒœë§Œ ì¡°í•­ ì‹œì‘ìœ¼ë¡œ ì¸ì‹
    # "ì œ1 ì¡°(..." í˜•íƒœëŠ” ì¡°í•­ ì°¸ì¡°ì´ë¯€ë¡œ ì œì™¸
    left_bracket = LEFT_BLACK_LENTICULAR_BRACKET  # ã€
    new_block_pattern = re.compile(
        r'^('
        rf'\uc81c\s*\d+\s*(\uc870|\uad00|\uc7a5|\uc808|\ud3b8)\s*{left_bracket}|'  # ì œ1ì¡° ã€
        r'\uc81c\s*\d+\s*(\uc870|\uad00|\uc7a5|\uc808|\ud3b8)\s*$|'  # ì œ1ì¡° (ì¤„ ë)
        r'\(\ubcc4\ud45c\s*\d+\)|'  # (ë³„í‘œ1) ë“±
        r'\(\ubcc4\s*\ud45c\s*\d*\)'  # (ë³„ í‘œ), (ë³„ í‘œ 1)
        r')'
    )

    # í•­ëª© ì‹œì‘ íŒ¨í„´ (ì›ë¬¸ì, ìˆ«ì ë¦¬ìŠ¤íŠ¸)
    # ì›ë¬¸ì íŒ¨í„´ì„ ìƒìˆ˜ ë°°ì—´ì—ì„œ ìƒì„±
    circled_pattern = ''.join(CIRCLED_NUMBERS)
    item_pattern = re.compile(
        rf'^([{circled_pattern}]|\d+\.)\s*'
    )

    def is_markdown_structure(stripped: str) -> bool:
        """ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ìš”ì†Œì¸ì§€ í™•ì¸"""
        if not stripped:
            return False
        return (stripped.startswith('#') or  # í—¤ë”©
                stripped.startswith('|') or  # í…Œì´ë¸”
                stripped.startswith('```') or  # ì½”ë“œë¸”ë¡
                stripped == '---' or stripped == '-----' or  # êµ¬ë¶„ì„ 
                stripped == '===')

    def is_new_section(stripped: str) -> bool:
        """ìƒˆ ì„¹ì…˜/í•­ëª© ì‹œì‘ì¸ì§€ í™•ì¸"""
        if not stripped:
            return False
        if is_markdown_structure(stripped):
            return True
        if new_block_pattern.match(stripped):
            return True
        if item_pattern.match(stripped):
            return True
        return False

    for line in lines:
        stripped = line.strip()

        # ë¹ˆ ì¤„ ì²˜ë¦¬
        if not stripped:
            pending_empty_lines += 1
            continue

        # ë¹ˆ ì¤„ ì´í›„ í…ìŠ¤íŠ¸ê°€ ë‚˜ì™”ì„ ë•Œ
        if pending_empty_lines > 0:
            # ìƒˆ ì„¹ì…˜/í•­ëª© ì‹œì‘ì´ë©´ ì´ì „ ë¬¸ë‹¨ì„ ì €ì¥í•˜ê³  ë¹ˆ ì¤„ ì¶”ê°€
            if is_new_section(stripped):
                if current_paragraph:
                    result_lines.append(' '.join(current_paragraph))
                    current_paragraph = []
                # 2ê°œ ì´ìƒì˜ ì—°ì† ë¹ˆ ì¤„ì€ í•˜ë‚˜ì˜ ë¹ˆ ì¤„ë¡œ ìœ ì§€
                if pending_empty_lines >= 2:
                    result_lines.append('')
            else:
                # ìƒˆ ì„¹ì…˜ì´ ì•„ë‹ˆë©´ í˜ì´ì§€ ë¶„ì ˆë¡œ ê°„ì£¼í•˜ê³  ë¬¸ë‹¨ ê³„ì† ì—°ê²°
                # (ë¹ˆ ì¤„ ë¬´ì‹œ)
                pass
            pending_empty_lines = 0

        # ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ìš”ì†ŒëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
        if is_markdown_structure(stripped):
            if current_paragraph:
                result_lines.append(' '.join(current_paragraph))
                current_paragraph = []
            result_lines.append(stripped)
            continue

        # ìƒˆ ë¸”ë¡ ì‹œì‘ (ì œ1ì¡°, ì œ1ê´€ ë“±) - ì´ì „ ë¬¸ë‹¨ ì €ì¥ í›„ ìƒˆ ì¤„
        if new_block_pattern.match(stripped):
            if current_paragraph:
                result_lines.append(' '.join(current_paragraph))
                current_paragraph = []
            result_lines.append(stripped)
            continue

        # í•­ëª© ì‹œì‘ (â‘ , â‘¡, 1., 2. ë“±) - ì´ì „ ë¬¸ë‹¨ ì €ì¥ í›„ ìƒˆ í•­ëª© ì‹œì‘
        if item_pattern.match(stripped):
            if current_paragraph:
                result_lines.append(' '.join(current_paragraph))
                current_paragraph = []
            # í•­ëª© ì‹œì‘ì´ë¯€ë¡œ ìƒˆ ë¬¸ë‹¨ìœ¼ë¡œ ì¶”ê°€
            current_paragraph.append(stripped)
            continue

        # ì¼ë°˜ í…ìŠ¤íŠ¸: í˜„ì¬ ë¬¸ë‹¨ì— ì¶”ê°€
        current_paragraph.append(stripped)

    # ë§ˆì§€ë§‰ ë¬¸ë‹¨ ì²˜ë¦¬
    if current_paragraph:
        result_lines.append(' '.join(current_paragraph))

    return '\n'.join(result_lines)


def convert_pdf_to_markdown_pymupdf(file_path: Path) -> str:
    """
    PDF íŒŒì¼ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜ (pymupdf4llm ì‚¬ìš©)

    Args:
        file_path: PDF íŒŒì¼ ê²½ë¡œ

    Returns:
        ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ í…ìŠ¤íŠ¸
    """
    # pymupdf4llmì„ ì‚¬ìš©í•˜ì—¬ PDFë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
    # - í‘œ, ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ êµ¬ì¡°ë¥¼ ì˜ ë³´ì¡´
    # - LLMì— ìµœì í™”ëœ ë§ˆí¬ë‹¤ìš´ ì¶œë ¥
    result = pymupdf4llm.to_markdown(str(file_path))

    # pymupdf4llm can return str or List[Dict] depending on options
    if isinstance(result, list):
        # If it's a list, join the text content
        markdown_content = "\n\n".join(
            item.get("text", "") if isinstance(item, dict) else str(item)
            for item in result
        )
    else:
        markdown_content = result

    # ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ ì •ë¦¬ (PDF ë ˆì´ì•„ì›ƒìœ¼ë¡œ ì¸í•œ ë¬¸ì¥ ì¤‘ê°„ ì¤„ë°”ê¿ˆ ì œê±°)
    markdown_content = normalize_line_breaks(markdown_content)

    return markdown_content


def convert_pdf_to_markdown_markitdown(file_path: Path) -> str:
    """
    PDF íŒŒì¼ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜ (markitdown ì‚¬ìš©)

    Azure Document Intelligenceê°€ ì„¤ì •ëœ ê²½ìš° OCR ê¸°ëŠ¥ í™œìš© ê°€ëŠ¥

    Args:
        file_path: PDF íŒŒì¼ ê²½ë¡œ

    Returns:
        ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ í…ìŠ¤íŠ¸
    """
    md = get_markitdown()
    result = md.convert(str(file_path))

    if result.text_content:
        return result.text_content

    raise ValueError(f"PDF \ubcc0\ud658 \uacb0\uacfc\uac00 \ube44\uc5b4\uc788\uc2b5\ub2c8\ub2e4: {file_path}")


def convert_pdf_to_markdown(file_path: Path, processor: Optional[str] = None) -> str:
    """
    PDF íŒŒì¼ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜

    Args:
        file_path: PDF íŒŒì¼ ê²½ë¡œ
        processor: ì‚¬ìš©í•  ì²˜ë¦¬ê¸° ("pymupdf4llm" ë˜ëŠ” "markitdown")
                   Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ PDF_PROCESSOR ê°’ ì‚¬ìš©

    Returns:
        ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ í…ìŠ¤íŠ¸
    """
    if processor is None:
        processor = PDF_PROCESSOR

    if processor == "markitdown":
        return convert_pdf_to_markdown_markitdown(file_path)
    else:
        # ê¸°ë³¸ê°’: pymupdf4llm
        return convert_pdf_to_markdown_pymupdf(file_path)


def convert_to_markdown(file_path: Path) -> tuple[str, str]:
    """
    íŒŒì¼ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜

    Args:
        file_path: ë³€í™˜í•  íŒŒì¼ ê²½ë¡œ

    Returns:
        (ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ , ì›ë³¸ íŒŒì¼ í˜•ì‹) íŠœí”Œ

    Raises:
        UnsupportedFormatException: ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹
        Exception: ë³€í™˜ ì‹¤íŒ¨
    """
    suffix = file_path.suffix.lower()

    # ì´ë¯¸ ë§ˆí¬ë‹¤ìš´ì¸ ê²½ìš°
    if suffix in {".md", ".markdown"}:
        content = file_path.read_text(encoding="utf-8")
        return content, "markdown"

    # í…ìŠ¤íŠ¸ íŒŒì¼ì¸ ê²½ìš°
    if suffix == ".txt":
        content = file_path.read_text(encoding="utf-8")
        return content, "plaintext"

    # PDF íŒŒì¼ì¸ ê²½ìš°
    if suffix == ".pdf":
        content = convert_pdf_to_markdown(file_path)
        return content, "pdf"

    # MarkItDownìœ¼ë¡œ ë³€í™˜
    md = get_markitdown()
    result = md.convert(str(file_path))

    if result.text_content:
        return result.text_content, suffix.lstrip(".")

    raise ValueError(f"ë³€í™˜ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {file_path}")


def detect_language(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì–¸ì–´ ê°ì§€ (langdetect ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)"""
    if not text or len(text.strip()) < 10:
        return "unknown"
    
    try:
        return detect(text)
    except LangDetectException:
        return "unknown"


def extract_title(content: str) -> str:
    """ì²« ë²ˆì§¸ í—¤ë”©ì„ ì œëª©ìœ¼ë¡œ ì¶”ì¶œ"""
    # H1 ì°¾ê¸°
    h1_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
    if h1_match:
        return h1_match.group(1).strip()
    
    # H2 ì°¾ê¸°
    h2_match = re.search(r'^##\s+(.+)$', content, re.MULTILINE)
    if h2_match:
        return h2_match.group(1).strip()
    
    # ì²« ì¤„
    first_line = content.strip().split('\n')[0].strip()
    if first_line:
        return first_line[:100]
    
    return "Untitled"


def extract_keywords(content: str, max_keywords: int = 10) -> list[str]:
    """
    ì½˜í…ì¸ ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    
    - í—¤ë”© í…ìŠ¤íŠ¸
    - ë³¼ë“œ/ì´íƒ¤ë¦­ í…ìŠ¤íŠ¸
    - ì½”ë“œ ë¸”ë¡ ì–¸ì–´
    - ë§í¬ í…ìŠ¤íŠ¸
    """
    keywords = set()
    
    # í—¤ë”© ì¶”ì¶œ
    headings = re.findall(r'^#{1,6}\s+(.+)$', content, re.MULTILINE)
    for h in headings:
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ ë‹¨ì–´ ì¶”ì¶œ
        words = re.findall(r'\b[\wê°€-í£]{2,}\b', h)
        keywords.update(words)
    
    # ë³¼ë“œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (**text** ë˜ëŠ” __text__)
    bold_texts = re.findall(r'\*\*(.+?)\*\*|__(.+?)__', content)
    for match in bold_texts:
        text = match[0] or match[1]
        words = re.findall(r'\b[\wê°€-í£]{2,}\b', text)
        keywords.update(words)
    
    # ì½”ë“œ ë¸”ë¡ ì–¸ì–´ ì¶”ì¶œ
    code_langs = re.findall(r'^```(\w+)', content, re.MULTILINE)
    keywords.update(code_langs)
    
    # ë§í¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    link_texts = re.findall(r'\[([^\]]+)\]\([^)]+\)', content)
    for lt in link_texts:
        if len(lt) > 2 and len(lt) < 50:
            keywords.add(lt.strip())
    
    # ë¶ˆìš©ì–´ ì œê±° (ê°„ë‹¨í•œ ëª©ë¡)
    stopwords = {
        'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',
        'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
        'could', 'should', 'may', 'might', 'can', 'this', 'that',
        'these', 'those', 'it', 'its', 'and', 'or', 'but', 'if',
        'then', 'else', 'when', 'where', 'how', 'what', 'why',
        'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ë˜ëŠ”', 'ì˜', 'ë¥¼', 'ì„',
    }
    
    filtered = [kw for kw in keywords if kw.lower() not in stopwords]
    
    # ê¸¸ì´ ê¸°ì¤€ ì •ë ¬ í›„ ìƒìœ„ Nê°œ ë°˜í™˜
    sorted_kw = sorted(filtered, key=lambda x: len(x), reverse=True)
    
    return sorted_kw[:max_keywords]


def detect_content_type(content: str) -> str:
    """ì½˜í…ì¸  íƒ€ì… ê°ì§€"""
    # ì½”ë“œ ë¸”ë¡ ë¹„ìœ¨
    code_blocks = re.findall(r'```[\s\S]*?```', content)
    code_length = sum(len(cb) for cb in code_blocks)
    code_ratio = code_length / len(content) if content else 0
    
    if code_ratio > 0.5:
        return "code"
    elif code_ratio > 0.2:
        return "tutorial"
    
    # ë¦¬ìŠ¤íŠ¸ ë¹„ìœ¨
    list_items = re.findall(r'^[-*]\s+', content, re.MULTILINE)
    numbered_items = re.findall(r'^\d+\.\s+', content, re.MULTILINE)
    list_count = len(list_items) + len(numbered_items)
    
    lines = content.count('\n') + 1
    if list_count > lines * 0.3:
        return "list"
    
    # í—¤ë”© ìˆ˜
    headings = re.findall(r'^#{1,6}\s+', content, re.MULTILINE)
    if len(headings) > 5:
        return "documentation"
    
    return "article"


def infer_domain(content: str, keywords: list[str]) -> tuple[str, str]:
    """
    ë„ë©”ì¸ ë° ì„œë¸Œë„ë©”ì¸ ì¶”ë¡ 
    
    Returns:
        (domain, sub_domain) íŠœí”Œ
    """
    content_lower = content.lower()
    keywords_lower = [kw.lower() for kw in keywords]
    
    # ë„ë©”ì¸ í‚¤ì›Œë“œ ë§¤í•‘
    domain_keywords = {
        "programming": [
            "python", "javascript", "typescript", "java", "rust", "go",
            "code", "function", "class", "api", "library", "framework",
            "ì½”ë“œ", "í•¨ìˆ˜", "í”„ë¡œê·¸ë˜ë°", "ê°œë°œ",
        ],
        "machine-learning": [
            "model", "training", "neural", "deep learning", "ai", "ml",
            "embedding", "transformer", "bert", "gpt", "llm",
            "ëª¨ë¸", "í•™ìŠµ", "ì¸ê³µì§€ëŠ¥", "ë”¥ëŸ¬ë‹", "ì„ë² ë”©",
        ],
        "data-science": [
            "data", "pandas", "numpy", "analysis", "visualization",
            "ë°ì´í„°", "ë¶„ì„", "ì‹œê°í™”", "í†µê³„",
        ],
        "devops": [
            "docker", "kubernetes", "ci/cd", "deploy", "container",
            "aws", "azure", "gcp", "cloud",
            "ë°°í¬", "ì»¨í…Œì´ë„ˆ", "í´ë¼ìš°ë“œ",
        ],
        "web": [
            "html", "css", "react", "vue", "angular", "frontend", "backend",
            "ì›¹", "í”„ë¡ íŠ¸ì—”ë“œ", "ë°±ì—”ë“œ",
        ],
        "database": [
            "sql", "nosql", "postgresql", "mongodb", "redis",
            "ë°ì´í„°ë² ì´ìŠ¤", "ì¿¼ë¦¬",
        ],
    }
    
    domain_scores: dict[str, int] = {}
    
    for domain, domain_kws in domain_keywords.items():
        score = 0
        for kw in domain_kws:
            if kw in content_lower:
                score += content_lower.count(kw)
            if kw in keywords_lower:
                score += 5  # í‚¤ì›Œë“œì— ìˆìœ¼ë©´ ê°€ì¤‘ì¹˜
        if score > 0:
            domain_scores[domain] = score
    
    if not domain_scores:
        return "general", ""
    
    # ìµœê³  ì ìˆ˜ ë„ë©”ì¸
    top_domain = max(domain_scores, key=lambda x: domain_scores[x])
    
    # ì„œë¸Œë„ë©”ì¸ì€ í•´ë‹¹ ë„ë©”ì¸ì˜ í‚¤ì›Œë“œ ì¤‘ ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ê²ƒ
    sub_domain = ""
    if top_domain in domain_keywords:
        sub_counts = {}
        for kw in domain_keywords[top_domain]:
            count = content_lower.count(kw)
            if count > 0:
                sub_counts[kw] = count
        if sub_counts:
            sub_domain = max(sub_counts, key=lambda x: sub_counts[x])
    
    return top_domain, sub_domain


def create_summary(content: str, max_length: int = 300) -> str:
    """ì²« ë²ˆì§¸ ë¬¸ë‹¨ì„ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©"""
    # YAML front matter ì œê±°
    if content.startswith("---"):
        parts = content.split("---", 2)
        if len(parts) >= 3:
            content = parts[2]
    
    # í—¤ë”© ì œê±°
    lines = content.strip().split('\n')
    paragraphs = []
    current = []
    
    for line in lines:
        stripped = line.strip()
        if stripped.startswith('#'):
            if current:
                paragraphs.append(' '.join(current))
                current = []
            continue
        if stripped == '':
            if current:
                paragraphs.append(' '.join(current))
                current = []
        else:
            current.append(stripped)
    
    if current:
        paragraphs.append(' '.join(current))
    
    # ì²« ë²ˆì§¸ ì˜ë¯¸ìˆëŠ” ë¬¸ë‹¨
    for p in paragraphs:
        # ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• ì œê±°
        clean = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', p)  # ë§í¬
        clean = re.sub(r'\*\*([^*]+)\*\*', r'\1', clean)  # ë³¼ë“œ
        clean = re.sub(r'\*([^*]+)\*', r'\1', clean)  # ì´íƒ¤ë¦­
        clean = re.sub(r'`([^`]+)`', r'\1', clean)  # ì¸ë¼ì¸ ì½”ë“œ
        
        if len(clean) > 30:
            if len(clean) > max_length:
                return clean[:max_length] + "..."
            return clean
    
    return ""


def prepare_document(
    input_path: Path,
    output_path: Optional[Path] = None,
) -> Path:
    """
    ë¬¸ì„œì— YAML front matter ë©”íƒ€ë°ì´í„° ì¶”ê°€

    Args:
        input_path: ì…ë ¥ íŒŒì¼ ê²½ë¡œ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
        output_path: ì¶œë ¥ ê²½ë¡œ (ê¸°ë³¸ê°’: OUTPUT_DIR)

    Returns:
        ìƒì„±ëœ íŒŒì¼ ê²½ë¡œ
    """
    # íŒŒì¼ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
    content, source_format = convert_to_markdown(input_path)

    # ê¸°ì¡´ front matterê°€ ìˆìœ¼ë©´ ì œê±°
    original_content = content
    if content.startswith("---"):
        parts = content.split("---", 2)
        if len(parts) >= 3:
            original_content = parts[2].strip()

    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    title = extract_title(original_content)
    keywords = extract_keywords(original_content)
    language = detect_language(original_content)
    content_type = detect_content_type(original_content)
    domain, sub_domain = infer_domain(original_content, keywords)
    summary = create_summary(original_content)

    # YAML front matter ìƒì„±
    metadata: dict[str, Any] = {
        "title": title,
        "domain": domain,
        "sub_domain": sub_domain,
        "keywords": keywords,
        "summary": summary,
        "language": language,
        "content_type": content_type,
        "source_file": input_path.name,
        "source_format": source_format,
        "prepared_at": datetime.now().isoformat(),
    }

    # ì¶œë ¥ ê²½ë¡œ ê²°ì • (í•­ìƒ .md í™•ì¥ì)
    if output_path is None:
        output_path = OUTPUT_DIR / input_path.with_suffix('.md').name

    output_path.parent.mkdir(parents=True, exist_ok=True)

    # ìƒˆ ì½˜í…ì¸  ì‘ì„±
    yaml_header = yaml.dump(metadata, allow_unicode=True, sort_keys=False, default_flow_style=False)
    new_content = f"---\n{yaml_header}---\n\n{original_content}"

    output_path.write_text(new_content, encoding="utf-8")

    return output_path


def process_all_documents(
    input_dir: Path = INPUT_DIR,
    output_dir: Path = OUTPUT_DIR,
) -> list[Path]:
    """
    ëª¨ë“  ì…ë ¥ ë¬¸ì„œ ì²˜ë¦¬ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)

    Args:
        input_dir: ì…ë ¥ ë””ë ‰í„°ë¦¬
        output_dir: ì¶œë ¥ ë””ë ‰í„°ë¦¬

    Returns:
        ìƒì„±ëœ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    input_dir = Path(input_dir)
    output_dir = Path(output_dir)

    if not input_dir.exists():
        print(f"âš ï¸ ì…ë ¥ ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {input_dir}")
        return []

    output_dir.mkdir(parents=True, exist_ok=True)

    # ì§€ì›ë˜ëŠ” ëª¨ë“  íŒŒì¼ ìˆ˜ì§‘
    all_files = [
        f for f in input_dir.iterdir()
        if f.is_file() and is_supported_file(f)
    ]

    if not all_files:
        print(f"âš ï¸ ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {input_dir}")
        print(f"   ì§€ì› í˜•ì‹: {', '.join(sorted(SUPPORTED_EXTENSIONS))}")
        return []

    # íŒŒì¼ í˜•ì‹ë³„ í†µê³„
    format_counts: dict[str, int] = {}
    for f in all_files:
        ext = f.suffix.lower()
        format_counts[ext] = format_counts.get(ext, 0) + 1

    print(f"\nğŸ“š ì²˜ë¦¬í•  ë¬¸ì„œ: {len(all_files)}ê°œ")
    print(f"   í˜•ì‹ë³„: {', '.join(f'{ext}({cnt})' for ext, cnt in sorted(format_counts.items()))}")
    print("=" * 50)

    results = []
    for i, file_path in enumerate(all_files, 1):
        print(f"\n[{i}/{len(all_files)}] {file_path.name}")

        try:
            output_path = output_dir / file_path.with_suffix('.md').name
            result = prepare_document(file_path, output_path)

            # ê²°ê³¼ í™•ì¸
            content = result.read_text(encoding="utf-8")
            if content.startswith("---"):
                parts = content.split("---", 2)
                if len(parts) >= 3:
                    meta = yaml.safe_load(parts[1])
                    print(f"   â€¢ ì œëª©: {meta.get('title', 'N/A')[:40]}...")
                    print(f"   â€¢ ë„ë©”ì¸: {meta.get('domain', 'N/A')}")
                    print(f"   â€¢ ì›ë³¸ í˜•ì‹: {meta.get('source_format', 'N/A')}")
                    print(f"   â€¢ í‚¤ì›Œë“œ: {', '.join(meta.get('keywords', [])[:5])}")
                    print(f"   âœ… ì €ì¥: {result.name}")

            results.append(result)

        except UnsupportedFormatException as e:
            print(f"   âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {e}")
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜: {e}")

    print("\n" + "=" * 50)
    print(f"âœ… ì™„ë£Œ: {len(results)}/{len(all_files)} ë¬¸ì„œ ì²˜ë¦¬ë¨")
    print(f"ğŸ“ ì¶œë ¥ ìœ„ì¹˜: {output_dir}")

    return results


def main() -> int:
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="ì…ë ¥ ë¬¸ì„œì— YAML front matter ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."
    )
    parser.add_argument(
        "--input-dir",
        type=Path,
        default=INPUT_DIR,
        help=f"ì…ë ¥ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸ê°’: {INPUT_DIR})",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=OUTPUT_DIR,
        help=f"ì¶œë ¥ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸ê°’: {OUTPUT_DIR})",
    )
    
    args = parser.parse_args()
    
    results = process_all_documents(
        input_dir=args.input_dir,
        output_dir=args.output_dir,
    )
    
    return 0 if results else 1


if __name__ == "__main__":
    try:
        exit(main())
    except KeyboardInterrupt:
        print("\n[ì¤‘ë‹¨ë¨]")
        exit(130)