"""
02_prepare_content.py
input_docs ë””ë ‰í„°ë¦¬ì˜ ë¬¸ì„œë¥¼ ì½ì–´ Phi-4 LLMìœ¼ë¡œ ë¶„ì„í•˜ê³ 
YAML front matterê°€ í¬í•¨ëœ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ prepared_contentsì— ì €ì¥í•˜ëŠ” ëª¨ë“ˆ
"""

import os
import sys
import json
import yaml
from pathlib import Path
from datetime import datetime
from typing import Any, Optional

import onnxruntime_genai as og  # type: ignore[import-untyped]


# ë””ë ‰í„°ë¦¬ ì„¤ì •
BASE_DIR = Path(__file__).parent
INPUT_DIR = BASE_DIR / "input_docs"
OUTPUT_DIR = BASE_DIR / "prepared_contents"
MODEL_DIR = BASE_DIR / "models" / "phi-4-onnx"


# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
ANALYSIS_PROMPT_TEMPLATE = """You are a document analyzer. Analyze the following document and extract metadata.

Document:
---
{content}
---

Please analyze this document and provide the following information in JSON format:
1. "domain": The main domain/field of this document (e.g., "technology", "science", "business", "education", etc.)
2. "sub_domain": A more specific sub-domain if applicable
3. "keywords": An array of 5-10 relevant keywords that describe the main topics
4. "summary": A brief 1-2 sentence summary of the document
5. "language": The primary language of the document (e.g., "ko", "en", "ja")
6. "content_type": The type of content (e.g., "article", "tutorial", "reference", "note", "report")
7. "difficulty_level": If applicable, the difficulty level ("beginner", "intermediate", "advanced")

Respond ONLY with valid JSON, no additional text."""


class Phi4ContentProcessor:
    """Phi-4 ONNX ëª¨ë¸ì„ ì‚¬ìš©í•œ ì½˜í…ì¸  ì²˜ë¦¬ê¸°"""
    
    def __init__(self, model_dir: Path = MODEL_DIR):
        """
        Args:
            model_dir: ONNX ëª¨ë¸ì´ ì €ì¥ëœ ë””ë ‰í„°ë¦¬
        """
        self.model_dir = Path(model_dir)
        self.model: Any = None
        self.tokenizer: Any = None
        self._load_model()
    
    def _load_model(self) -> None:
        """ONNX ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_dir}")
        
        if not self.model_dir.exists():
            raise FileNotFoundError(
                f"ëª¨ë¸ ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {self.model_dir}\n"
                "ë¨¼ì € 01_download_model.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”."
            )
        
        try:
            Model = getattr(og, "Model")
            Tokenizer = getattr(og, "Tokenizer")
            self.model = Model(str(self.model_dir))
            self.tokenizer = Tokenizer(self.model)
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            raise RuntimeError(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    
    def generate_response(
        self,
        prompt: str,
        max_length: int = 2048,
        temperature: float = 0.7,
        top_p: float = 0.9,
    ) -> str:
        """
        Phi-4 ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„±
        
        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            max_length: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„
            top_p: Top-p ìƒ˜í”Œë§ ê°’
            
        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        import numpy as np
        
        # Phi-4 chat í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        chat_prompt = f"<|user|>\n{prompt}<|end|>\n<|assistant|>\n"
        
        # í† í°í™”
        input_tokens = self.tokenizer.encode(chat_prompt)
        
        # ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
        GeneratorParams = getattr(og, "GeneratorParams")
        Generator = getattr(og, "Generator")
        params = GeneratorParams(self.model)
        params.set_search_options(
            max_length=max_length,
            temperature=temperature,
            top_p=top_p,
        )
        
        # í…ìŠ¤íŠ¸ ìƒì„±
        generator = Generator(self.model, params)
        
        # ì…ë ¥ í† í° ì¶”ê°€ (ìƒˆ API ë°©ì‹)
        generator.append_tokens(input_tokens)
        
        output_tokens = []
        
        while not generator.is_done():
            generator.generate_next_token()
            
            new_token = generator.get_next_tokens()[0]
            output_tokens.append(new_token)
        
        # ë””ì½”ë”©
        response = self.tokenizer.decode(np.array(output_tokens, dtype=np.int32))
        
        # ì¢…ë£Œ í† í° ì œê±°
        if "<|end|>" in response:
            response = response.split("<|end|>")[0]
        
        return response.strip()
    
    def analyze_document(self, content: str) -> dict:
        """
        ë¬¸ì„œ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        
        Args:
            content: ë¬¸ì„œ ë‚´ìš©
            
        Returns:
            ë¶„ì„ëœ ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        # ë„ˆë¬´ ê¸´ ë¬¸ì„œëŠ” ì•ë¶€ë¶„ë§Œ ì‚¬ìš© (í† í° ì œí•œ)
        max_content_length = 8000
        truncated_content = content[:max_content_length]
        if len(content) > max_content_length:
            truncated_content += "\n...[truncated]..."
        
        prompt = ANALYSIS_PROMPT_TEMPLATE.format(content=truncated_content)
        
        try:
            response = self.generate_response(prompt, temperature=0.3)
            
            # JSON íŒŒì‹± ì‹œë„
            # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            json_start = response.find("{")
            json_end = response.rfind("}") + 1
            
            if json_start != -1 and json_end > json_start:
                json_str = response[json_start:json_end]
                metadata = json.loads(json_str)
                return metadata
            else:
                print(f"âš ï¸ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì‘ë‹µ: {response[:200]}...")
                return self._get_default_metadata()
                
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            return self._get_default_metadata()
        except Exception as e:
            print(f"âš ï¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return self._get_default_metadata()
    
    def _get_default_metadata(self) -> dict:
        """ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ë°˜í™˜"""
        return {
            "domain": "unknown",
            "sub_domain": None,
            "keywords": [],
            "summary": "",
            "language": "unknown",
            "content_type": "document",
            "difficulty_level": None,
        }


def read_document(file_path: Path) -> str:
    """íŒŒì¼ì—ì„œ ë¬¸ì„œ ë‚´ìš© ì½ê¸°"""
    encodings = ["utf-8", "utf-8-sig", "cp949", "euc-kr", "latin-1"]
    
    for encoding in encodings:
        try:
            return file_path.read_text(encoding=encoding)
        except UnicodeDecodeError:
            continue
    
    raise ValueError(f"íŒŒì¼ ì¸ì½”ë”©ì„ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")


def create_markdown_with_frontmatter(
    original_content: str,
    metadata: dict,
    source_file: str,
) -> str:
    """
    YAML front matterê°€ í¬í•¨ëœ ë§ˆí¬ë‹¤ìš´ ìƒì„±
    
    Args:
        original_content: ì›ë³¸ ë¬¸ì„œ ë‚´ìš©
        metadata: ë¶„ì„ëœ ë©”íƒ€ë°ì´í„°
        source_file: ì›ë³¸ íŒŒì¼ëª…
        
    Returns:
        YAML front matterê°€ í¬í•¨ëœ ë§ˆí¬ë‹¤ìš´ ë¬¸ìì—´
    """
    # ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€ ì •ë³´ í¬í•¨
    frontmatter = {
        "title": Path(source_file).stem,
        "source_file": source_file,
        "processed_at": datetime.now().isoformat(),
        "domain": metadata.get("domain", "unknown"),
        "sub_domain": metadata.get("sub_domain"),
        "keywords": metadata.get("keywords", []),
        "summary": metadata.get("summary", ""),
        "language": metadata.get("language", "unknown"),
        "content_type": metadata.get("content_type", "document"),
        "difficulty_level": metadata.get("difficulty_level"),
    }
    
    # None ê°’ ì œê±°
    frontmatter = {k: v for k, v in frontmatter.items() if v is not None}
    
    # YAML front matter ìƒì„±
    yaml_content = yaml.dump(
        frontmatter,
        default_flow_style=False,
        allow_unicode=True,
        sort_keys=False,
    )
    
    # ë§ˆí¬ë‹¤ìš´ ì¡°í•©
    markdown = f"""---
{yaml_content.strip()}
---

{original_content}
"""
    
    return markdown


def process_documents(
    input_dir: Path = INPUT_DIR,
    output_dir: Path = OUTPUT_DIR,
    model_dir: Path = MODEL_DIR,
):
    """
    input_docsì˜ ëª¨ë“  ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ì—¬ prepared_contentsì— ì €ì¥
    
    Args:
        input_dir: ì…ë ¥ ë¬¸ì„œ ë””ë ‰í„°ë¦¬
        output_dir: ì¶œë ¥ ë””ë ‰í„°ë¦¬
        model_dir: ëª¨ë¸ ë””ë ‰í„°ë¦¬
    """
    input_dir = Path(input_dir)
    output_dir = Path(output_dir)
    
    # ë””ë ‰í„°ë¦¬ í™•ì¸/ìƒì„±
    if not input_dir.exists():
        print(f"âš ï¸ ì…ë ¥ ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒì„±í•©ë‹ˆë‹¤: {input_dir}")
        input_dir.mkdir(parents=True, exist_ok=True)
        print("ğŸ“ input_docs ë””ë ‰í„°ë¦¬ì— ì²˜ë¦¬í•  ë¬¸ì„œë¥¼ ë„£ì–´ì£¼ì„¸ìš”.")
        return
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ì²˜ë¦¬í•  íŒŒì¼ í™•ì¥ì
    supported_extensions = {".txt", ".md", ".markdown", ".rst", ".html", ".htm"}
    
    # ì…ë ¥ íŒŒì¼ ëª©ë¡
    input_files = [
        f for f in input_dir.iterdir()
        if f.is_file() and f.suffix.lower() in supported_extensions
    ]
    
    if not input_files:
        print(f"âš ï¸ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤: {input_dir}")
        print(f"   ì§€ì› í˜•ì‹: {', '.join(supported_extensions)}")
        return
    
    print(f"\nğŸ“š ì²˜ë¦¬í•  ë¬¸ì„œ: {len(input_files)}ê°œ")
    print("=" * 50)
    
    # ëª¨ë¸ ë¡œë“œ
    processor = Phi4ContentProcessor(model_dir)
    
    # ê° ë¬¸ì„œ ì²˜ë¦¬
    success_count = 0
    for i, input_file in enumerate(input_files, 1):
        print(f"\n[{i}/{len(input_files)}] ì²˜ë¦¬ ì¤‘: {input_file.name}")
        
        try:
            # ë¬¸ì„œ ì½ê¸°
            content = read_document(input_file)
            print(f"   ğŸ“– ë¬¸ì„œ ê¸¸ì´: {len(content)} ë¬¸ì")
            
            # LLMìœ¼ë¡œ ë¶„ì„
            print("   ğŸ” LLM ë¶„ì„ ì¤‘...")
            metadata = processor.analyze_document(content)
            print(f"   âœ“ ë„ë©”ì¸: {metadata.get('domain', 'unknown')}")
            print(f"   âœ“ í‚¤ì›Œë“œ: {', '.join(metadata.get('keywords', [])[:5])}")
            
            # ë§ˆí¬ë‹¤ìš´ ìƒì„±
            markdown_content = create_markdown_with_frontmatter(
                original_content=content,
                metadata=metadata,
                source_file=input_file.name,
            )
            
            # ì¶œë ¥ íŒŒì¼ ì €ì¥
            output_file = output_dir / f"{input_file.stem}.md"
            output_file.write_text(markdown_content, encoding="utf-8")
            print(f"   ğŸ’¾ ì €ì¥: {output_file.name}")
            
            success_count += 1
            
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜: {e}")
    
    print("\n" + "=" * 50)
    print(f"âœ… ì™„ë£Œ: {success_count}/{len(input_files)} ë¬¸ì„œ ì²˜ë¦¬ë¨")
    print(f"ğŸ“ ì¶œë ¥ ìœ„ì¹˜: {output_dir}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ YAML front matterê°€ í¬í•¨ëœ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."
    )
    parser.add_argument(
        "--input-dir",
        type=Path,
        default=INPUT_DIR,
        help=f"ì…ë ¥ ë¬¸ì„œ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸ê°’: {INPUT_DIR})",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=OUTPUT_DIR,
        help=f"ì¶œë ¥ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸ê°’: {OUTPUT_DIR})",
    )
    parser.add_argument(
        "--model-dir",
        type=Path,
        default=MODEL_DIR,
        help=f"ëª¨ë¸ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸ê°’: {MODEL_DIR})",
    )
    
    args = parser.parse_args()
    
    process_documents(
        input_dir=args.input_dir,
        output_dir=args.output_dir,
        model_dir=args.model_dir,
    )


if __name__ == "__main__":
    main()
