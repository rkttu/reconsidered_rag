"""
04_build_vector_db.py
Module that reads parquet files from chunked_data, generates BGE-M3 embeddings,
and compiles them into a local vector database based on sqlite-vec

Features:
- BGE-M3 Dense vector (1024-dimension) based search
- Utilizes sqlite-vec extension (vector similarity search)
- Stores metadata and vectors together
- Parquet export portable to Milvus/Qdrant, etc.
"""

import json
import sqlite3
import struct
from pathlib import Path
from datetime import datetime
from typing import Any, Optional

import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import sqlite_vec
import torch
from FlagEmbedding import BGEM3FlagModel  # type: ignore[import-untyped]


# ë””ë ‰í„°ë¦¬ ì„¤ì •
BASE_DIR = Path(__file__).parent
INPUT_DIR = BASE_DIR / "chunked_data"
OUTPUT_DIR = BASE_DIR / "vector_db"

# BGE-M3 ì„¤ì •
EMBEDDING_DIM = 1024  # BGE-M3 Dense ë²¡í„° ì°¨ì›


def get_device_info() -> tuple[str, bool]:
    """Return available device and FP16 support status"""
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name(0)
        return device_name, True
    elif torch.backends.mps.is_available():
        return "Apple MPS", False
    else:
        return "CPU", False


class VectorDBBuilder:
    """sqlite-vec based vector DB builder"""

    def __init__(self, db_path: Path):
        self.db_path = db_path
        self.conn: Optional[sqlite3.Connection] = None
        self.model: Optional[Any] = None

    def _load_model(self) -> None:
        """BGE-M3 ëª¨ë¸ ë¡œë“œ"""
        device_name, use_fp16 = get_device_info()

        if use_fp16:
            print(f"ğŸ”„ BGE-M3 ëª¨ë¸ ë¡œë”© ì¤‘... (GPU: {device_name}, FP16)")
        else:
            print(f"ğŸ”„ BGE-M3 ëª¨ë¸ ë¡œë”© ì¤‘... ({device_name}, FP32)")

        self.model = BGEM3FlagModel("BAAI/bge-m3", use_fp16=use_fp16)
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    def _init_db(self) -> None:
        """sqlite-vec DB ì´ˆê¸°í™”"""
        self.db_path.parent.mkdir(parents=True, exist_ok=True)

        self.conn = sqlite3.connect(str(self.db_path))
        self.conn.enable_load_extension(True)
        sqlite_vec.load(self.conn)
        self.conn.enable_load_extension(False)

        # ë©”íƒ€ë°ì´í„° í…Œì´ë¸”
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS chunks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                chunk_id TEXT UNIQUE NOT NULL,
                content_hash TEXT,
                source_file TEXT,
                chunk_index INTEGER,
                chunk_text TEXT,
                chunk_type TEXT,
                heading_level INTEGER,
                heading_text TEXT,
                parent_heading TEXT,
                section_path TEXT,  -- JSON ë°°ì—´
                table_headers TEXT,  -- JSON ë°°ì—´
                table_row_count INTEGER,
                domain TEXT,
                sub_domain TEXT,
                keywords TEXT,
                language TEXT,
                content_type TEXT,
                version INTEGER,
                created_at TEXT,
                updated_at TEXT,
                embedded_at TEXT
            )
        """)

        # ë²¡í„° í…Œì´ë¸” (sqlite-vec ê°€ìƒ í…Œì´ë¸”)
        self.conn.execute(f"""
            CREATE VIRTUAL TABLE IF NOT EXISTS chunk_vectors USING vec0(
                chunk_id TEXT PRIMARY KEY,
                embedding FLOAT[{EMBEDDING_DIM}]
            )
        """)

        # ì¸ë±ìŠ¤
        self.conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_chunks_source_file
            ON chunks(source_file)
        """)
        self.conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_chunks_domain
            ON chunks(domain)
        """)

        self.conn.commit()
        print(f"âœ… DB ì´ˆê¸°í™” ì™„ë£Œ: {self.db_path}")

    def _get_embeddings(self, texts: list[str]) -> np.ndarray:
        """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì˜ Dense ì„ë² ë”© ê³„ì‚°"""
        if not texts or self.model is None:
            return np.array([])

        result = self.model.encode(texts, batch_size=32)
        return result["dense_vecs"].astype(np.float32)

    def _serialize_vector(self, vec: np.ndarray) -> bytes:
        """numpy ë²¡í„°ë¥¼ sqlite-vecìš© bytesë¡œ ë³€í™˜"""
        return struct.pack(f"{len(vec)}f", *vec)

    def build(self, input_dir: Path = INPUT_DIR) -> dict:
        """
        ë²¡í„° DB ë¹Œë“œ

        Args:
            input_dir: chunked_data ë””ë ‰í„°ë¦¬

        Returns:
            ë¹Œë“œ í†µê³„
        """
        input_dir = Path(input_dir)

        if not input_dir.exists():
            print(f"âš ï¸ ì…ë ¥ ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {input_dir}")
            return {"error": "Input directory not found"}

        parquet_files = list(input_dir.glob("*.parquet"))
        if not parquet_files:
            print(f"âš ï¸ parquet íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {input_dir}")
            return {"error": "No parquet files found"}

        print(f"\nğŸ“¦ ë²¡í„° DB ë¹Œë“œ ì‹œì‘")
        print(f"   ì…ë ¥: {len(parquet_files)}ê°œ parquet íŒŒì¼")
        print("=" * 50)

        # ëª¨ë¸ ë° DB ì´ˆê¸°í™”
        self._load_model()
        self._init_db()

        # íƒ€ì… ê°€ë“œ: connì´ Noneì´ë©´ ì˜¤ë¥˜
        if self.conn is None:
            raise RuntimeError("DB ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")

        stats = {
            "total_chunks": 0,
            "embedded_chunks": 0,
            "skipped_chunks": 0,
            "files_processed": 0,
        }

        now = datetime.now().isoformat()

        for i, pq_file in enumerate(parquet_files, 1):
            print(f"\n[{i}/{len(parquet_files)}] {pq_file.name}")

            try:
                df = pd.read_parquet(pq_file)
                print(f"   ğŸ“– ì²­í¬ ìˆ˜: {len(df)}")

                # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” chunk_id í™•ì¸
                chunk_ids = df["chunk_id"].tolist()
                placeholders = ",".join(["?"] * len(chunk_ids))
                existing = set(
                    row[0] for row in self.conn.execute(
                        f"SELECT chunk_id FROM chunks WHERE chunk_id IN ({placeholders})",
                        chunk_ids
                    ).fetchall()
                )

                new_chunks = df[~df["chunk_id"].isin(existing)]
                stats["skipped_chunks"] += len(existing)

                if new_chunks.empty:
                    print(f"   â­ï¸ ëª¨ë“  ì²­í¬ê°€ ì´ë¯¸ ì¡´ì¬í•¨")
                    stats["files_processed"] += 1
                    continue

                print(f"   ğŸ” ìƒˆ ì²­í¬: {len(new_chunks)}ê°œ, ìŠ¤í‚µ: {len(existing)}ê°œ")

                # ì„ë² ë”© ìƒì„±
                texts = new_chunks["chunk_text"].tolist()
                embeddings = self._get_embeddings(texts)

                # DBì— ì‚½ì…
                for idx, (_, row) in enumerate(new_chunks.iterrows()):
                    # ë©”íƒ€ë°ì´í„° í…Œì´ë¸”
                    # section_pathì™€ table_headersê°€ numpy ë°°ì—´ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ listë¡œ ë³€í™˜
                    section_path = row.get("section_path", [])
                    if hasattr(section_path, "tolist"):
                        section_path = section_path.tolist()
                    table_headers = row.get("table_headers", [])
                    if hasattr(table_headers, "tolist"):
                        table_headers = table_headers.tolist()

                    self.conn.execute("""
                        INSERT OR REPLACE INTO chunks (
                            chunk_id, content_hash, source_file, chunk_index,
                            chunk_text, chunk_type, heading_level, heading_text,
                            parent_heading, section_path, table_headers, table_row_count,
                            domain, sub_domain, keywords, language, content_type,
                            version, created_at, updated_at, embedded_at
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        row["chunk_id"],
                        row.get("content_hash", ""),
                        row.get("source_file", ""),
                        row.get("chunk_index", 0),
                        row["chunk_text"],
                        row.get("chunk_type", ""),
                        row.get("heading_level", 0),
                        row.get("heading_text", ""),
                        row.get("parent_heading", ""),
                        json.dumps(section_path, ensure_ascii=False),
                        json.dumps(table_headers, ensure_ascii=False),
                        row.get("table_row_count", 0),
                        row.get("domain", ""),
                        row.get("sub_domain", ""),
                        row.get("keywords", ""),
                        row.get("language", ""),
                        row.get("content_type", ""),
                        row.get("version", 1),
                        row.get("created_at", now),
                        row.get("updated_at", now),
                        now,
                    ))

                    # ë²¡í„° í…Œì´ë¸”
                    vec_bytes = self._serialize_vector(embeddings[idx])
                    self.conn.execute(
                        "INSERT INTO chunk_vectors (chunk_id, embedding) VALUES (?, ?)",
                        (row["chunk_id"], vec_bytes)
                    )

                    stats["embedded_chunks"] += 1

                self.conn.commit()
                stats["total_chunks"] += len(df)
                stats["files_processed"] += 1
                print(f"   âœ… ì™„ë£Œ")

            except Exception as e:
                print(f"   âŒ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()

        print("\n" + "=" * 50)
        print(f"âœ… ë¹Œë“œ ì™„ë£Œ")
        print(f"   ğŸ“Š ì´ ì²­í¬: {stats['total_chunks']}")
        print(f"   ğŸ†• ì„ë² ë”© ìƒì„±: {stats['embedded_chunks']}")
        print(f"   â­ï¸ ìŠ¤í‚µ: {stats['skipped_chunks']}")
        print(f"   ğŸ’¾ DB ìœ„ì¹˜: {self.db_path}")

        return stats

    def export_for_milvus(self, output_path: Path) -> Path:
        """
        Milvus/Qdrant ë“±ìœ¼ë¡œ ì´ì‹ ê°€ëŠ¥í•œ Parquet íŒŒì¼ ë‚´ë³´ë‚´ê¸°

        ë²¡í„°ë¥¼ float32 ë°°ì—´ë¡œ í¬í•¨í•˜ì—¬ ë‹¤ë¥¸ DBì—ì„œ ì§ì ‘ import ê°€ëŠ¥
        """
        if self.conn is None:
            raise RuntimeError("DBê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        print(f"\nğŸ“¤ ë²¡í„° DB ë‚´ë³´ë‚´ê¸° ì¤‘...")

        # ëª¨ë“  ë°ì´í„° ì¡°íšŒ
        chunks_df = pd.read_sql_query(
            "SELECT * FROM chunks ORDER BY id", self.conn
        )

        # ë²¡í„° ì¡°íšŒ ë° ë³€í™˜
        vectors = []
        for chunk_id in chunks_df["chunk_id"]:
            result = self.conn.execute(
                "SELECT embedding FROM chunk_vectors WHERE chunk_id = ?",
                (chunk_id,)
            ).fetchone()

            if result:
                vec_bytes = result[0]
                vec = np.frombuffer(vec_bytes, dtype=np.float32)
                vectors.append(vec.tolist())
            else:
                vectors.append([0.0] * EMBEDDING_DIM)

        chunks_df["embedding"] = vectors

        # Parquet ì €ì¥ (ë²¡í„° í¬í•¨)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        chunks_df.to_parquet(output_path, compression="zstd")

        print(f"âœ… ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_path}")
        print(f"   ğŸ“Š ì²­í¬ ìˆ˜: {len(chunks_df)}")
        print(f"   ğŸ“ ë²¡í„° ì°¨ì›: {EMBEDDING_DIM}")

        return output_path

    def search(
        self,
        query: str,
        top_k: int = 5,
        domain_filter: Optional[str] = None,
    ) -> list[dict]:
        """
        ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            domain_filter: ë„ë©”ì¸ í•„í„° (ì„ íƒ)

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if self.conn is None or self.model is None:
            raise RuntimeError("DB ë˜ëŠ” ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self._get_embeddings([query])[0]
        query_bytes = self._serialize_vector(query_embedding)

        # ë²¡í„° ê²€ìƒ‰ (sqlite-vec knn ì¿¼ë¦¬)
        # í•„í„°ë§ì„ ìœ„í•´ ë” ë§ì´ ê°€ì ¸ì˜´
        limit = top_k * 2
        results = self.conn.execute("""
            SELECT
                v.chunk_id,
                v.distance,
                c.chunk_text,
                c.source_file,
                c.heading_text,
                c.section_path,
                c.domain
            FROM chunk_vectors v
            JOIN chunks c ON v.chunk_id = c.chunk_id
            WHERE v.embedding MATCH ?
              AND k = ?
            ORDER BY v.distance
        """, (query_bytes, limit)).fetchall()

        # ê²°ê³¼ ë³€í™˜ ë° í•„í„°ë§
        output = []
        for row in results:
            if domain_filter and row[6] != domain_filter:
                continue

            output.append({
                "chunk_id": row[0],
                "distance": row[1],
                "similarity": 1 - row[1],  # cosine distance â†’ similarity
                "chunk_text": row[2],
                "source_file": row[3],
                "heading_text": row[4],
                "section_path": json.loads(row[5]) if row[5] else [],
                "domain": row[6],
            })

            if len(output) >= top_k:
                break

        return output

    def close(self) -> None:
        """DB ì—°ê²° ì¢…ë£Œ"""
        if self.conn:
            self.conn.close()
            self.conn = None


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(
        description="chunked_dataë¥¼ sqlite-vec ë²¡í„° DBë¡œ ì»´íŒŒì¼í•©ë‹ˆë‹¤."
    )
    parser.add_argument(
        "--input-dir",
        type=Path,
        default=INPUT_DIR,
        help=f"ì…ë ¥ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸ê°’: {INPUT_DIR})",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=OUTPUT_DIR,
        help=f"ì¶œë ¥ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸ê°’: {OUTPUT_DIR})",
    )
    parser.add_argument(
        "--db-name",
        type=str,
        default="vectors.db",
        help="DB íŒŒì¼ëª… (ê¸°ë³¸ê°’: vectors.db)",
    )
    parser.add_argument(
        "--export-parquet",
        action="store_true",
        help="Milvus/Qdrant ì´ì‹ìš© Parquet íŒŒì¼ ë‚´ë³´ë‚´ê¸°",
    )
    parser.add_argument(
        "--test-search",
        type=str,
        default=None,
        help="ë¹Œë“œ í›„ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ìˆ˜í–‰",
    )

    args = parser.parse_args()

    db_path = args.output_dir / args.db_name
    builder = VectorDBBuilder(db_path)

    try:
        # ë¹Œë“œ
        stats = builder.build(args.input_dir)

        if "error" in stats:
            return 1

        # Parquet ë‚´ë³´ë‚´ê¸°
        if args.export_parquet:
            export_path = args.output_dir / "vectors_export.parquet"
            builder.export_for_milvus(export_path)

        # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
        if args.test_search:
            print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰: '{args.test_search}'")
            print("-" * 50)

            results = builder.search(args.test_search, top_k=3)
            for i, r in enumerate(results, 1):
                print(f"\n[{i}] ìœ ì‚¬ë„: {r['similarity']:.4f}")
                print(f"    íŒŒì¼: {r['source_file']}")
                print(f"    ì„¹ì…˜: {' > '.join(r['section_path'])}")
                print(f"    ë‚´ìš©: {r['chunk_text'][:100]}...")

        return 0

    finally:
        builder.close()


if __name__ == "__main__":
    try:
        exit(main())
    except KeyboardInterrupt:
        print("\n[ì¤‘ë‹¨ë¨]")
        exit(130)
