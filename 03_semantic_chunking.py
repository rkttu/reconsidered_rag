"""
03_semantic_chunking.py
prepared_contentsì˜ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ BGE-M3 ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬
ì‹œë§¨í‹± ì²­í‚¹ìœ¼ë¡œ ë¶„í• í•˜ê³  parquet íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ëª¨ë“ˆ

íŠ¹ì§•:
- Markdown êµ¬ì¡° íŒŒì‹± (heading hierarchy ë³´ì¡´)
- BGE-M3 ì„ë² ë”© ê¸°ë°˜ ì‹œë§¨í‹± ìœ ì‚¬ë„ ì²­í‚¹
- zstd ì••ì¶• ë° ì¦ë¶„ ì—…ë°ì´íŠ¸ ì§€ì›
"""

import re
import json
import yaml
import hashlib
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, field
from typing import Any, Optional

import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import mistune
import torch
from FlagEmbedding import BGEM3FlagModel  # type: ignore[import-untyped]


def get_device_info() -> tuple[str, bool]:
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ ë° FP16 ì§€ì› ì—¬ë¶€ ë°˜í™˜

    Returns:
        (device_name, use_fp16) íŠœí”Œ
    """
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name(0)
        return device_name, True
    elif torch.backends.mps.is_available():
        # Apple Silicon (M1/M2/M3)
        return "Apple MPS", False  # MPSëŠ” FP16ì´ ì œí•œì 
    else:
        return "CPU", False


# ë””ë ‰í„°ë¦¬ ì„¤ì •
BASE_DIR = Path(__file__).parent
INPUT_DIR = BASE_DIR / "prepared_contents"
OUTPUT_DIR = BASE_DIR / "chunked_data"

# ì‹œë§¨í‹± ì²­í‚¹ ì„¤ì •
SIMILARITY_THRESHOLD = 0.5  # ìœ ì‚¬ë„ ì„ê³„ê°’ (ë‚®ì„ìˆ˜ë¡ ë” ë§ì€ ì²­í¬ ë¶„í• )
MIN_CHUNK_SIZE = 50  # ìµœì†Œ ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)
MAX_CHUNK_SIZE = 1500  # ìµœëŒ€ ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)


@dataclass
class MarkdownSection:
    """ë§ˆí¬ë‹¤ìš´ ì„¹ì…˜ ì •ë³´"""
    text: str
    heading_level: int = 0  # 0 = ì¼ë°˜ í…ìŠ¤íŠ¸, 1-6 = í—¤ë”© ë ˆë²¨
    heading_text: str = ""
    section_path: list[str] = field(default_factory=list)  # ê³„ì¸µ ê²½ë¡œ ë°°ì—´
    element_type: str = "paragraph"  # header, paragraph, list, code, blockquote, table
    line_start: int = 0
    line_end: int = 0
    # í‘œ ì „ìš© ë©”íƒ€ë°ì´í„°
    table_headers: list[str] = field(default_factory=list)
    table_row_count: int = 0


@dataclass
class SemanticChunk:
    """ì‹œë§¨í‹± ì²­í¬ ì •ë³´"""
    text: str
    heading_level: int = 0
    heading_text: str = ""
    parent_heading: str = ""
    section_path: list[str] = field(default_factory=list)  # ê³„ì¸µ ê²½ë¡œ ë°°ì—´
    chunk_type: str = "paragraph"
    start_line: int = 0
    end_line: int = 0
    # í‘œ ì „ìš© ë©”íƒ€ë°ì´í„°
    table_headers: list[str] = field(default_factory=list)
    table_row_count: int = 0


class MarkdownParser:
    """Mistune ê¸°ë°˜ ë§ˆí¬ë‹¤ìš´ íŒŒì„œ"""
    
    def __init__(self):
        self.sections: list[MarkdownSection] = []
        self.current_headings: dict[int, str] = {}  # level -> heading text
    
    def parse(self, markdown_text: str) -> list[MarkdownSection]:
        """
        ë§ˆí¬ë‹¤ìš´ì„ íŒŒì‹±í•˜ì—¬ ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        
        Args:
            markdown_text: ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸
            
        Returns:
            MarkdownSection ë¦¬ìŠ¤íŠ¸
        """
        self.sections = []
        self.current_headings = {}
        
        # mistuneìœ¼ë¡œ AST íŒŒì‹± (table í”ŒëŸ¬ê·¸ì¸ í™œì„±í™”)
        md = mistune.create_markdown(renderer=None, plugins=['table'])
        tokens = md(markdown_text)
        
        if tokens is None:
            tokens = []
        
        # ë¼ì¸ ë²ˆí˜¸ ì¶”ì ì„ ìœ„í•´ ì›ë³¸ í…ìŠ¤íŠ¸ ë¶„í• 
        lines = markdown_text.split('\n')
        current_line = 0
        
        for token in tokens:
            if isinstance(token, dict):
                self._process_token(token, lines, current_line)
        
        # í† í° ê¸°ë°˜ íŒŒì‹±ì´ ë¹„ì–´ìˆìœ¼ë©´ ë¼ì¸ ê¸°ë°˜ íŒŒì‹±ìœ¼ë¡œ í´ë°±
        if not self.sections:
            self._fallback_parse(markdown_text)
        
        return self.sections
    
    def _process_token(self, token: dict, lines: list[str], line_offset: int) -> None:
        """í† í° ì²˜ë¦¬"""
        token_type = token.get('type', '')
        
        if token_type == 'heading':
            level = token.get('attrs', {}).get('level', 1)
            # childrenì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            children = token.get('children', [])
            text = self._extract_text_from_children(children)
            
            # í˜„ì¬ í—¤ë”© ì—…ë°ì´íŠ¸
            self.current_headings[level] = text
            # í•˜ìœ„ ë ˆë²¨ ì´ˆê¸°í™”
            for l in range(level + 1, 7):
                self.current_headings.pop(l, None)
            
            section_path = self._build_section_path()
            
            self.sections.append(MarkdownSection(
                text=f"{'#' * level} {text}",
                heading_level=level,
                heading_text=text,
                section_path=section_path,
                element_type="header",
            ))
        
        elif token_type == 'paragraph':
            children = token.get('children', [])
            text = self._extract_text_from_children(children)
            if text.strip():
                self.sections.append(MarkdownSection(
                    text=text,
                    heading_level=0,
                    heading_text=self._get_current_heading(),
                    section_path=self._build_section_path(),
                    element_type="paragraph",
                ))
        
        elif token_type == 'list':
            items = token.get('children', [])
            list_text = self._extract_list_text(items)
            if list_text.strip():
                self.sections.append(MarkdownSection(
                    text=list_text,
                    heading_level=0,
                    heading_text=self._get_current_heading(),
                    section_path=self._build_section_path(),
                    element_type="list",
                ))
        
        elif token_type == 'code_block':
            raw = token.get('raw', '')
            if raw.strip():
                self.sections.append(MarkdownSection(
                    text=raw,
                    heading_level=0,
                    heading_text=self._get_current_heading(),
                    section_path=self._build_section_path(),
                    element_type="code",
                ))
        
        elif token_type == 'block_quote':
            children = token.get('children', [])
            text = self._extract_text_from_children(children)
            if text.strip():
                self.sections.append(MarkdownSection(
                    text=text,
                    heading_level=0,
                    heading_text=self._get_current_heading(),
                    section_path=self._build_section_path(),
                    element_type="blockquote",
                ))
        
        elif token_type == 'table':
            # í‘œ ì²˜ë¦¬: ì „ì²´ í‘œë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ìœ ì§€
            table_text, headers, row_count = self._extract_table(token)
            if table_text.strip():
                self.sections.append(MarkdownSection(
                    text=table_text,
                    heading_level=0,
                    heading_text=self._get_current_heading(),
                    section_path=self._build_section_path(),
                    element_type="table",
                    table_headers=headers,
                    table_row_count=row_count,
                ))
    
    def _extract_text_from_children(self, children: list) -> str:
        """children í† í°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        texts = []
        for child in children:
            if isinstance(child, dict):
                if child.get('type') == 'text':
                    texts.append(child.get('raw', ''))
                elif 'children' in child:
                    texts.append(self._extract_text_from_children(child['children']))
                elif 'raw' in child:
                    texts.append(child.get('raw', ''))
        return ''.join(texts)
    
    def _extract_list_text(self, items: list) -> str:
        """ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        texts = []
        for i, item in enumerate(items):
            if isinstance(item, dict):
                children = item.get('children', [])
                item_text = self._extract_text_from_children(children)
                texts.append(f"- {item_text}")
        return '\n'.join(texts)
    
    def _extract_table(self, token: dict) -> tuple[str, list[str], int]:
        """
        í‘œ í† í°ì—ì„œ í…ìŠ¤íŠ¸, í—¤ë”, í–‰ ìˆ˜ ì¶”ì¶œ
        
        Returns:
            (table_markdown, headers, row_count)
        """
        headers: list[str] = []
        rows: list[list[str]] = []
        
        children = token.get('children', [])
        for child in children:
            if not isinstance(child, dict):
                continue
            
            child_type = child.get('type', '')
            
            if child_type == 'table_head':
                # í‘œ í—¤ë” ì¶”ì¶œ - table_headê°€ ì§ì ‘ table_cellì„ í¬í•¨
                head_cells = child.get('children', [])
                for cell in head_cells:
                    if isinstance(cell, dict) and cell.get('type') == 'table_cell':
                        cell_text = self._extract_text_from_children(
                            cell.get('children', [])
                        )
                        headers.append(cell_text.strip())
            
            elif child_type == 'table_body':
                # í‘œ ë³¸ë¬¸ ì¶”ì¶œ
                body_rows = child.get('children', [])
                for row in body_rows:
                    if isinstance(row, dict) and row.get('type') == 'table_row':
                        cells = row.get('children', [])
                        row_data = []
                        for cell in cells:
                            if isinstance(cell, dict) and cell.get('type') == 'table_cell':
                                cell_text = self._extract_text_from_children(
                                    cell.get('children', [])
                                )
                                row_data.append(cell_text.strip())
                        if row_data:
                            rows.append(row_data)
        
        # ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ìœ¼ë¡œ ì¬êµ¬ì„±
        md_lines = []
        if headers:
            md_lines.append('| ' + ' | '.join(headers) + ' |')
            md_lines.append('| ' + ' | '.join(['---'] * len(headers)) + ' |')
        
        for row in rows:
            # í—¤ë” ìˆ˜ì— ë§ì¶° íŒ¨ë”©
            padded_row = row + [''] * (len(headers) - len(row)) if headers else row
            md_lines.append('| ' + ' | '.join(padded_row) + ' |')
        
        table_text = '\n'.join(md_lines)
        return table_text, headers, len(rows)
    
    def _build_section_path(self) -> list[str]:
        """í˜„ì¬ ì„¹ì…˜ ê²½ë¡œë¥¼ ë°°ì—´ë¡œ ìƒì„±"""
        parts = []
        for level in sorted(self.current_headings.keys()):
            parts.append(self.current_headings[level])
        return parts
    
    def _get_current_heading(self) -> str:
        """í˜„ì¬ ê°€ì¥ ê¹Šì€ í—¤ë”© ë°˜í™˜"""
        if self.current_headings:
            max_level = max(self.current_headings.keys())
            return self.current_headings[max_level]
        return ""
    
    def _fallback_parse(self, markdown_text: str) -> None:
        """ë¼ì¸ ê¸°ë°˜ í´ë°± íŒŒì‹±"""
        lines = markdown_text.split('\n')
        current_headings: dict[int, str] = {}
        current_paragraph: list[str] = []
        
        def flush_paragraph():
            if current_paragraph:
                text = '\n'.join(current_paragraph).strip()
                if text:
                    section_path = [
                        h for l, h in sorted(current_headings.items())
                    ]
                    current_heading = current_headings.get(
                        max(current_headings.keys()) if current_headings else 0, ""
                    )
                    
                    # ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
                    is_list = all(
                        line.strip().startswith(('-', '*', '1.', '2.', '3.'))
                        for line in current_paragraph if line.strip()
                    )
                    
                    self.sections.append(MarkdownSection(
                        text=text,
                        heading_level=0,
                        heading_text=current_heading,
                        section_path=section_path,
                        element_type="list" if is_list else "paragraph",
                    ))
                current_paragraph.clear()
        
        # í‘œ ê°ì§€ ë° ì²˜ë¦¬ í•¨ìˆ˜
        def is_table_line(line: str) -> bool:
            """í‘œ ë¼ì¸ì¸ì§€ í™•ì¸ (| ë¡œ ì‹œì‘í•˜ê³  ëë‚¨)"""
            s = line.strip()
            return s.startswith('|') and s.endswith('|')
        
        def is_separator_line(line: str) -> bool:
            """í‘œ êµ¬ë¶„ì„ ì¸ì§€ í™•ì¸ (|---|---|)"""
            s = line.strip()
            if not (s.startswith('|') and s.endswith('|')):
                return False
            # ì¤‘ê°„ì— ---ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€
            return bool(re.match(r'^\|[\s\-:|]+\|$', s))
        
        def flush_table(table_lines: list[str], start_idx: int):
            """í‘œë¥¼ ì„¹ì…˜ìœ¼ë¡œ ì¶”ê°€"""
            if not table_lines:
                return
            
            table_text = '\n'.join(table_lines)
            headers: list[str] = []
            row_count = 0
            
            # ì²« ì¤„ì—ì„œ í—¤ë” ì¶”ì¶œ
            if table_lines:
                first_line = table_lines[0].strip()
                if first_line.startswith('|') and first_line.endswith('|'):
                    cells = [c.strip() for c in first_line[1:-1].split('|')]
                    headers = [c for c in cells if c]
            
            # êµ¬ë¶„ì„  ì œì™¸í•˜ê³  ë°ì´í„° í–‰ ìˆ˜ ê³„ì‚°
            for tl in table_lines[2:]:  # í—¤ë”, êµ¬ë¶„ì„  ì œì™¸
                if is_table_line(tl) and not is_separator_line(tl):
                    row_count += 1
            
            section_path = [
                h for l, h in sorted(current_headings.items())
            ]
            current_heading = current_headings.get(
                max(current_headings.keys()) if current_headings else 0, ""
            )
            
            self.sections.append(MarkdownSection(
                text=table_text,
                heading_level=0,
                heading_text=current_heading,
                section_path=section_path,
                element_type="table",
                line_start=start_idx,
                line_end=start_idx + len(table_lines) - 1,
                table_headers=headers,
                table_row_count=row_count,
            ))
        
        i = 0
        while i < len(lines):
            line = lines[i]
            stripped = line.strip()
            
            # í—¤ë”© ê°ì§€
            heading_match = re.match(r'^(#{1,6})\s+(.+)$', stripped)
            if heading_match:
                flush_paragraph()
                
                level = len(heading_match.group(1))
                heading_text = heading_match.group(2).strip()
                
                # í—¤ë”© ì—…ë°ì´íŠ¸
                current_headings[level] = heading_text
                for l in list(current_headings.keys()):
                    if l > level:
                        del current_headings[l]
                
                section_path = [
                    h for l, h in sorted(current_headings.items())
                ]
                
                self.sections.append(MarkdownSection(
                    text=stripped,
                    heading_level=level,
                    heading_text=heading_text,
                    section_path=section_path,
                    element_type="header",
                    line_start=i,
                    line_end=i,
                ))
                i += 1
            
            # í‘œ ê°ì§€ (| ë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸)
            elif is_table_line(stripped):
                flush_paragraph()
                
                # ì—°ì†ëœ í‘œ ë¼ì¸ ìˆ˜ì§‘
                table_lines = [line]
                table_start = i
                i += 1
                
                while i < len(lines) and is_table_line(lines[i].strip()):
                    table_lines.append(lines[i])
                    i += 1
                
                # ìµœì†Œ 2ì¤„ ì´ìƒì´ë©´ í‘œë¡œ ì²˜ë¦¬ (í—¤ë” + êµ¬ë¶„ì„ )
                if len(table_lines) >= 2:
                    flush_table(table_lines, table_start)
                else:
                    # í‘œê°€ ì•„ë‹ˆë©´ ì¼ë°˜ ë¬¸ë‹¨ìœ¼ë¡œ
                    current_paragraph.extend(table_lines)
            
            elif stripped == '':
                flush_paragraph()
                i += 1
            
            else:
                current_paragraph.append(line)
                i += 1
        
        flush_paragraph()


class SemanticChunker:
    """BGE-M3 ê¸°ë°˜ ì‹œë§¨í‹± ì²­í‚¹"""
    
    def __init__(
        self,
        similarity_threshold: float = SIMILARITY_THRESHOLD,
        min_chunk_size: int = MIN_CHUNK_SIZE,
        max_chunk_size: int = MAX_CHUNK_SIZE,
    ):
        self.similarity_threshold = similarity_threshold
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
        self.model: Any = None
        self.parser = MarkdownParser()
        self._load_model()
    
    def _load_model(self) -> None:
        """BGE-M3 ëª¨ë¸ ë¡œë“œ (GPU ì—†ìœ¼ë©´ CPU í´ë°±)"""
        device_name, use_fp16 = get_device_info()

        if use_fp16:
            print(f"ğŸ”„ BGE-M3 ëª¨ë¸ ë¡œë”© ì¤‘... (GPU: {device_name}, FP16)")
        else:
            print(f"ğŸ”„ BGE-M3 ëª¨ë¸ ë¡œë”© ì¤‘... ({device_name}, FP32)")

        self.model = BGEM3FlagModel("BAAI/bge-m3", use_fp16=use_fp16)
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    def _get_embeddings(self, texts: list[str]) -> np.ndarray:
        """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì˜ ì„ë² ë”© ê³„ì‚°"""
        if not texts:
            return np.array([])
        
        result = self.model.encode(texts, batch_size=32)
        return result["dense_vecs"]
    
    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8))
    
    def chunk_document(self, content: str, metadata: dict) -> list[SemanticChunk]:
        """
        ë¬¸ì„œë¥¼ ì‹œë§¨í‹± ì²­í¬ë¡œ ë¶„í• 
        
        Args:
            content: ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ë‚´ìš©
            metadata: ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
            
        Returns:
            SemanticChunk ë¦¬ìŠ¤íŠ¸
        """
        # 1. ë§ˆí¬ë‹¤ìš´ íŒŒì‹±
        sections = self.parser.parse(content)
        
        if not sections:
            return [SemanticChunk(
                text=content,
                chunk_type="document",
            )]
        
        # 2. ì„¹ì…˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì„ë² ë”©
        section_texts = [s.text for s in sections]
        
        if len(section_texts) <= 1:
            # ì„¹ì…˜ì´ 1ê°œ ì´í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return [
                SemanticChunk(
                    text=s.text,
                    heading_level=s.heading_level,
                    heading_text=s.heading_text,
                    parent_heading=self._get_parent_heading(s),
                    section_path=s.section_path,
                    chunk_type=s.element_type,
                )
                for s in sections
            ]
        
        embeddings = self._get_embeddings(section_texts)
        
        # 3. ìœ ì‚¬ë„ ê¸°ë°˜ ì²­í‚¹
        chunks: list[SemanticChunk] = []
        current_sections: list[MarkdownSection] = [sections[0]]
        current_text_length = len(sections[0].text)
        
        for i in range(1, len(sections)):
            section = sections[i]
            prev_section = sections[i - 1]
            
            # í—¤ë”©ì€ í•­ìƒ ìƒˆ ì²­í¬ ì‹œì‘
            if section.heading_level > 0:
                # ì´ì „ ì²­í¬ ì €ì¥
                if current_sections:
                    chunks.append(self._merge_sections(current_sections))
                current_sections = [section]
                current_text_length = len(section.text)
                continue
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            similarity = self._cosine_similarity(embeddings[i], embeddings[i - 1])
            
            # ì²­í¬ ë¶„í•  ì¡°ê±´:
            # 1. ìœ ì‚¬ë„ê°€ ì„ê³„ê°’ ë¯¸ë§Œ
            # 2. ë˜ëŠ” ìµœëŒ€ í¬ê¸° ì´ˆê³¼
            should_split = (
                similarity < self.similarity_threshold
                or current_text_length + len(section.text) > self.max_chunk_size
            )
            
            if should_split and current_text_length >= self.min_chunk_size:
                # ì´ì „ ì²­í¬ ì €ì¥
                chunks.append(self._merge_sections(current_sections))
                current_sections = [section]
                current_text_length = len(section.text)
            else:
                # í˜„ì¬ ì²­í¬ì— ì¶”ê°€
                current_sections.append(section)
                current_text_length += len(section.text)
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_sections:
            chunks.append(self._merge_sections(current_sections))
        
        return chunks
    
    def _merge_sections(self, sections: list[MarkdownSection]) -> SemanticChunk:
        """ì—¬ëŸ¬ ì„¹ì…˜ì„ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ë³‘í•©"""
        if not sections:
            return SemanticChunk(text="")
        
        # ì²« ë²ˆì§¸ ì„¹ì…˜ì˜ ë©”íƒ€ë°ì´í„° ì‚¬ìš©
        first = sections[0]
        
        # í…ìŠ¤íŠ¸ ë³‘í•©
        merged_text = "\n\n".join(s.text for s in sections)
        
        # íƒ€ì… ê²°ì • (ê°€ì¥ ë§ì€ íƒ€ì… ë˜ëŠ” ì²« ë²ˆì§¸)
        types = [s.element_type for s in sections]
        chunk_type = max(set(types), key=types.count)
        
        # í‘œì¸ ê²½ìš° ë©”íƒ€ë°ì´í„° ì „ë‹¬
        table_headers: list[str] = []
        table_row_count = 0
        for s in sections:
            if s.element_type == "table" and s.table_headers:
                table_headers = s.table_headers
                table_row_count = s.table_row_count
                break
        
        return SemanticChunk(
            text=merged_text,
            heading_level=first.heading_level,
            heading_text=first.heading_text,
            parent_heading=self._get_parent_heading(first),
            section_path=first.section_path,
            chunk_type=chunk_type,
            table_headers=table_headers,
            table_row_count=table_row_count,
        )
    
    def _get_parent_heading(self, section: MarkdownSection) -> str:
        """ë¶€ëª¨ í—¤ë”© ì¶”ì¶œ"""
        path = section.section_path
        if len(path) >= 2:
            return path[-2]
        return ""


def parse_markdown_with_frontmatter(file_path: Path) -> tuple[dict, str]:
    """YAML front matterê°€ ìˆëŠ” ë§ˆí¬ë‹¤ìš´ íŒŒì¼ íŒŒì‹±"""
    text = file_path.read_text(encoding="utf-8")
    
    if text.startswith("---"):
        parts = text.split("---", 2)
        if len(parts) >= 3:
            try:
                metadata = yaml.safe_load(parts[1])
                content = parts[2].strip()
                return metadata or {}, content
            except yaml.YAMLError:
                pass
    
    return {}, text


def generate_chunk_id(source_file: str, chunk_index: int, chunk_text: str) -> str:
    """ì²­í¬ ê³ ìœ  ID ìƒì„±"""
    hash_input = f"{source_file}:{chunk_index}:{chunk_text[:100]}"
    return hashlib.md5(hash_input.encode()).hexdigest()[:16]


def generate_content_hash(text: str) -> str:
    """ì½˜í…ì¸  í•´ì‹œ ìƒì„±"""
    return hashlib.sha256(text.encode()).hexdigest()


def generate_source_hash(content: str, metadata: dict) -> str:
    """ì†ŒìŠ¤ íŒŒì¼ ì „ì²´ í•´ì‹œ"""
    hash_input = content + json.dumps(metadata, sort_keys=True, ensure_ascii=False)
    return hashlib.sha256(hash_input.encode()).hexdigest()[:32]


def load_existing_parquet(file_path: Path) -> tuple[pd.DataFrame | None, dict]:
    """ê¸°ì¡´ parquet íŒŒì¼ ë¡œë“œ"""
    if not file_path.exists():
        return None, {}
    
    try:
        table = pq.read_table(file_path)
        df = table.to_pandas()
        
        file_meta = table.schema.metadata or {}
        metadata = {
            "version": int(file_meta.get(b"version", b"0")),
            "source_hash": file_meta.get(b"source_hash", b"").decode(),
            "created_at": file_meta.get(b"created_at", b"").decode(),
            "updated_at": file_meta.get(b"updated_at", b"").decode(),
        }
        return df, metadata
    except Exception as e:
        print(f"âš ï¸ ê¸°ì¡´ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, {}


def check_needs_update(existing_meta: dict, new_source_hash: str) -> bool:
    """ì—…ë°ì´íŠ¸ í•„ìš” ì—¬ë¶€ í™•ì¸"""
    if not existing_meta:
        return True
    return existing_meta.get("source_hash") != new_source_hash


def merge_chunks(
    existing_df: pd.DataFrame | None,
    new_records: list[dict],
    existing_meta: dict,
) -> tuple[list[dict], dict]:
    """ì¦ë¶„ ì—…ë°ì´íŠ¸ ë³‘í•©"""
    stats = {"added": 0, "updated": 0, "unchanged": 0, "deleted": 0}
    
    if existing_df is None or existing_df.empty:
        stats["added"] = len(new_records)
        return new_records, stats
    
    existing_hashes = set(existing_df["content_hash"].tolist()) if "content_hash" in existing_df.columns else set()
    existing_chunk_ids = set(existing_df["chunk_id"].tolist())
    
    merged = []
    new_hashes = set()
    new_chunk_ids = set()
    
    for record in new_records:
        content_hash = record["content_hash"]
        chunk_id = record["chunk_id"]
        new_hashes.add(content_hash)
        new_chunk_ids.add(chunk_id)
        
        if content_hash in existing_hashes:
            existing_row = existing_df[existing_df["content_hash"] == content_hash].iloc[0]
            record["version"] = int(existing_row["version"])
            record["created_at"] = existing_row["created_at"]
            stats["unchanged"] += 1
        else:
            if chunk_id in existing_chunk_ids:
                existing_row = existing_df[existing_df["chunk_id"] == chunk_id].iloc[0]
                record["version"] = int(existing_row["version"]) + 1
                record["created_at"] = existing_row["created_at"]
                stats["updated"] += 1
            else:
                stats["added"] += 1
        
        merged.append(record)
    
    deleted_chunk_ids = existing_chunk_ids - new_chunk_ids
    stats["deleted"] = len(deleted_chunk_ids)
    
    return merged, stats


def process_documents(
    input_dir: Path = INPUT_DIR,
    output_dir: Path = OUTPUT_DIR,
    similarity_threshold: float = SIMILARITY_THRESHOLD,
):
    """ë¬¸ì„œ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
    input_dir = Path(input_dir)
    output_dir = Path(output_dir)
    
    if not input_dir.exists():
        print(f"âš ï¸ ì…ë ¥ ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {input_dir}")
        return
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    md_files = list(input_dir.glob("*.md"))
    
    if not md_files:
        print(f"âš ï¸ ì²˜ë¦¬í•  ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {input_dir}")
        return
    
    print(f"\nğŸ“š ì²˜ë¦¬í•  ë¬¸ì„œ: {len(md_files)}ê°œ")
    print("=" * 50)
    
    # ì²­ì»¤ ë¡œë“œ
    chunker = SemanticChunker(similarity_threshold=similarity_threshold)
    
    success_count = 0
    total_chunks = 0
    
    for i, md_file in enumerate(md_files, 1):
        print(f"\n[{i}/{len(md_files)}] ì²˜ë¦¬ ì¤‘: {md_file.name}")
        
        try:
            # ë§ˆí¬ë‹¤ìš´ íŒŒì‹±
            metadata, content = parse_markdown_with_frontmatter(md_file)
            print(f"   ğŸ“– ë¬¸ì„œ ê¸¸ì´: {len(content)} ë¬¸ì")
            
            # ì†ŒìŠ¤ í•´ì‹œ ê³„ì‚°
            source_hash = generate_source_hash(content, metadata)
            output_file = output_dir / f"{md_file.stem}.parquet"
            
            # ê¸°ì¡´ íŒŒì¼ í™•ì¸
            existing_df, existing_meta = load_existing_parquet(output_file)
            
            # ë³€ê²½ ì—¬ë¶€ í™•ì¸
            if not check_needs_update(existing_meta, source_hash):
                print(f"   â­ï¸ ë³€ê²½ ì—†ìŒ, ìŠ¤í‚µ")
                success_count += 1
                if existing_df is not None:
                    total_chunks += len(existing_df)
                continue
            
            # ì‹œë§¨í‹± ì²­í‚¹
            print("   ğŸ” ì‹œë§¨í‹± ì²­í‚¹ ì¤‘...")
            chunks = chunker.chunk_document(content, metadata)
            print(f"   âœ“ ìƒì„±ëœ ì²­í¬: {len(chunks)}ê°œ")
            
            now = datetime.now().isoformat()
            new_version = existing_meta.get("version", 0) + 1
            created_at = existing_meta.get("created_at") or now
            
            # ë ˆì½”ë“œ êµ¬ì„±
            records = []
            for idx, chunk in enumerate(chunks):
                chunk_id = generate_chunk_id(md_file.name, idx, chunk.text)
                content_hash = generate_content_hash(chunk.text)
                
                records.append({
                    "chunk_id": chunk_id,
                    "content_hash": content_hash,
                    "source_file": md_file.name,
                    "chunk_index": idx,
                    "chunk_text": chunk.text,
                    "chunk_type": chunk.chunk_type,
                    # Hierarchy ì •ë³´ (section_pathëŠ” ë°°ì—´)
                    "heading_level": chunk.heading_level,
                    "heading_text": chunk.heading_text,
                    "parent_heading": chunk.parent_heading,
                    "section_path": chunk.section_path,  # list[str]
                    # í‘œ ë©”íƒ€ë°ì´í„°
                    "table_headers": chunk.table_headers,  # list[str]
                    "table_row_count": chunk.table_row_count,
                    # ë©”íƒ€ë°ì´í„°
                    "domain": metadata.get("domain", ""),
                    "sub_domain": metadata.get("sub_domain", ""),
                    "keywords": json.dumps(metadata.get("keywords", []), ensure_ascii=False),
                    "language": metadata.get("language", ""),
                    "content_type": metadata.get("content_type", ""),
                    # ë²„ì „ ê´€ë¦¬
                    "version": 1,
                    "created_at": now,
                    "updated_at": now,
                })
            
            # ì¦ë¶„ ì—…ë°ì´íŠ¸ ë³‘í•©
            merged_records, update_stats = merge_chunks(existing_df, records, existing_meta)
            
            if existing_df is not None:
                print(f"   ğŸ“Š ì¦ë¶„ ì—…ë°ì´íŠ¸: ì¶”ê°€ {update_stats['added']}, ìˆ˜ì • {update_stats['updated']}, "
                      f"ìœ ì§€ {update_stats['unchanged']}, ì‚­ì œ {update_stats['deleted']}")
            
            # Parquet ì €ì¥
            df = pd.DataFrame(merged_records)
            
            schema = pa.schema([
                ("chunk_id", pa.string()),
                ("content_hash", pa.string()),
                ("source_file", pa.string()),
                ("chunk_index", pa.int32()),
                ("chunk_text", pa.string()),
                ("chunk_type", pa.string()),
                # Hierarchy (section_pathëŠ” ë°°ì—´)
                ("heading_level", pa.int32()),
                ("heading_text", pa.string()),
                ("parent_heading", pa.string()),
                ("section_path", pa.list_(pa.string())),  # ë°°ì—´ íƒ€ì…
                # Table metadata
                ("table_headers", pa.list_(pa.string())),  # í‘œ ì»¬ëŸ¼ í—¤ë”
                ("table_row_count", pa.int32()),  # í‘œ í–‰ ìˆ˜
                # Metadata
                ("domain", pa.string()),
                ("sub_domain", pa.string()),
                ("keywords", pa.string()),
                ("language", pa.string()),
                ("content_type", pa.string()),
                # Version
                ("version", pa.int32()),
                ("created_at", pa.string()),
                ("updated_at", pa.string()),
            ])
            
            file_metadata = {
                b"version": str(new_version).encode(),
                b"source_hash": source_hash.encode(),
                b"created_at": created_at.encode(),
                b"updated_at": now.encode(),
                b"schema_version": b"2.0",
                b"chunking_method": b"semantic_bge_m3",
            }
            
            table = pa.Table.from_pandas(df, schema=schema)
            table = table.replace_schema_metadata(file_metadata)
            
            pq.write_table(
                table,
                output_file,
                compression="zstd",
                compression_level=3,
            )
            
            print(f"   ğŸ’¾ ì €ì¥: {output_file.name} (v{new_version}, zstd ì••ì¶•)")
            
            success_count += 1
            total_chunks += len(chunks)
            
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "=" * 50)
    print(f"âœ… ì™„ë£Œ: {success_count}/{len(md_files)} ë¬¸ì„œ ì²˜ë¦¬ë¨")
    print(f"ğŸ“Š ì´ ì²­í¬ ìˆ˜: {total_chunks}ê°œ")
    print(f"ğŸ“ ì¶œë ¥ ìœ„ì¹˜: {output_dir}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œë¥¼ ì‹œë§¨í‹± ì²­í‚¹í•˜ì—¬ parquetìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."
    )
    parser.add_argument(
        "--input-dir",
        type=Path,
        default=INPUT_DIR,
        help=f"ì…ë ¥ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸ê°’: {INPUT_DIR})",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=OUTPUT_DIR,
        help=f"ì¶œë ¥ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸ê°’: {OUTPUT_DIR})",
    )
    parser.add_argument(
        "--similarity-threshold",
        type=float,
        default=SIMILARITY_THRESHOLD,
        help=f"ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: {SIMILARITY_THRESHOLD})",
    )
    
    args = parser.parse_args()
    
    process_documents(
        input_dir=args.input_dir,
        output_dir=args.output_dir,
        similarity_threshold=args.similarity_threshold,
    )


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n[ì¤‘ë‹¨ë¨]")
        exit(130)
