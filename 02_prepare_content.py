"""
02_prepare_content.py
ì…ë ¥ ë¬¸ì„œì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  YAML front matterë¥¼ ì¶”ê°€í•˜ëŠ” ëª¨ë“ˆ

íŠ¹ì§•:
- Microsoft markitdownì„ í™œìš©í•œ ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ ì§€ì›
  (Word, Excel, PowerPoint, PDF, HTML, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë“±)
- ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ë¶„ì„ì„ í†µí•œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
- í‚¤ì›Œë“œ ìë™ ì¶”ì¶œ (í—¤ë”©, ë³¼ë“œ, ë§í¬ í…ìŠ¤íŠ¸ ë“±)
- ì–¸ì–´ ê°ì§€ ì§€ì›
- YAML front matter ìƒì„±
"""

import re
from pathlib import Path
from datetime import datetime
from typing import Optional

import yaml
from langdetect import detect, LangDetectException
from markitdown import MarkItDown, UnsupportedFormatException


# ë””ë ‰í„°ë¦¬ ì„¤ì •
BASE_DIR = Path(__file__).parent
INPUT_DIR = BASE_DIR / "input_docs"
OUTPUT_DIR = BASE_DIR / "prepared_contents"

# MarkItDown ì§€ì› íŒŒì¼ í™•ì¥ì
SUPPORTED_EXTENSIONS = {
    # ë¬¸ì„œ
    ".pdf", ".docx", ".doc", ".pptx", ".ppt", ".xlsx", ".xls",
    # ì›¹/í…ìŠ¤íŠ¸
    ".html", ".htm", ".xml", ".json", ".csv",
    # ë§ˆí¬ë‹¤ìš´/í…ìŠ¤íŠ¸
    ".md", ".markdown", ".txt", ".rst",
    # ì´ë¯¸ì§€ (EXIF/OCR)
    ".jpg", ".jpeg", ".png", ".gif", ".bmp", ".webp", ".tiff",
    # ì˜¤ë””ì˜¤ (ìŒì„± ì¸ì‹)
    ".mp3", ".wav", ".m4a", ".ogg", ".flac",
    # ë¹„ë””ì˜¤ (ìë§‰ ì¶”ì¶œ)
    ".mp4", ".mkv", ".avi", ".mov", ".webm",
    # ì½”ë“œ/ê¸°íƒ€
    ".py", ".js", ".ts", ".java", ".c", ".cpp", ".cs", ".go", ".rs",
    ".ipynb",  # Jupyter Notebook
    ".zip",  # Archive (ë‚´ë¶€ íŒŒì¼ ì²˜ë¦¬)
}

# MarkItDown ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_markitdown_instance: Optional[MarkItDown] = None


def get_markitdown() -> MarkItDown:
    """MarkItDown ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _markitdown_instance
    if _markitdown_instance is None:
        _markitdown_instance = MarkItDown()
    return _markitdown_instance


def is_supported_file(file_path: Path) -> bool:
    """íŒŒì¼ì´ ì§€ì›ë˜ëŠ” í˜•ì‹ì¸ì§€ í™•ì¸"""
    return file_path.suffix.lower() in SUPPORTED_EXTENSIONS


def convert_to_markdown(file_path: Path) -> tuple[str, str]:
    """
    íŒŒì¼ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜

    Args:
        file_path: ë³€í™˜í•  íŒŒì¼ ê²½ë¡œ

    Returns:
        (ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ , ì›ë³¸ íŒŒì¼ í˜•ì‹) íŠœí”Œ

    Raises:
        UnsupportedFormatException: ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹
        Exception: ë³€í™˜ ì‹¤íŒ¨
    """
    suffix = file_path.suffix.lower()

    # ì´ë¯¸ ë§ˆí¬ë‹¤ìš´ì¸ ê²½ìš°
    if suffix in {".md", ".markdown"}:
        content = file_path.read_text(encoding="utf-8")
        return content, "markdown"

    # í…ìŠ¤íŠ¸ íŒŒì¼ì¸ ê²½ìš°
    if suffix == ".txt":
        content = file_path.read_text(encoding="utf-8")
        return content, "plaintext"

    # MarkItDownìœ¼ë¡œ ë³€í™˜
    md = get_markitdown()
    result = md.convert(str(file_path))

    if result.text_content:
        return result.text_content, suffix.lstrip(".")

    raise ValueError(f"ë³€í™˜ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {file_path}")


def detect_language(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì–¸ì–´ ê°ì§€ (langdetect ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)"""
    if not text or len(text.strip()) < 10:
        return "unknown"
    
    try:
        return detect(text)
    except LangDetectException:
        return "unknown"


def extract_title(content: str) -> str:
    """ì²« ë²ˆì§¸ í—¤ë”©ì„ ì œëª©ìœ¼ë¡œ ì¶”ì¶œ"""
    # H1 ì°¾ê¸°
    h1_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
    if h1_match:
        return h1_match.group(1).strip()
    
    # H2 ì°¾ê¸°
    h2_match = re.search(r'^##\s+(.+)$', content, re.MULTILINE)
    if h2_match:
        return h2_match.group(1).strip()
    
    # ì²« ì¤„
    first_line = content.strip().split('\n')[0].strip()
    if first_line:
        return first_line[:100]
    
    return "Untitled"


def extract_keywords(content: str, max_keywords: int = 10) -> list[str]:
    """
    ì½˜í…ì¸ ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    
    - í—¤ë”© í…ìŠ¤íŠ¸
    - ë³¼ë“œ/ì´íƒ¤ë¦­ í…ìŠ¤íŠ¸
    - ì½”ë“œ ë¸”ë¡ ì–¸ì–´
    - ë§í¬ í…ìŠ¤íŠ¸
    """
    keywords = set()
    
    # í—¤ë”© ì¶”ì¶œ
    headings = re.findall(r'^#{1,6}\s+(.+)$', content, re.MULTILINE)
    for h in headings:
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ ë‹¨ì–´ ì¶”ì¶œ
        words = re.findall(r'\b[\wê°€-í£]{2,}\b', h)
        keywords.update(words)
    
    # ë³¼ë“œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (**text** ë˜ëŠ” __text__)
    bold_texts = re.findall(r'\*\*(.+?)\*\*|__(.+?)__', content)
    for match in bold_texts:
        text = match[0] or match[1]
        words = re.findall(r'\b[\wê°€-í£]{2,}\b', text)
        keywords.update(words)
    
    # ì½”ë“œ ë¸”ë¡ ì–¸ì–´ ì¶”ì¶œ
    code_langs = re.findall(r'^```(\w+)', content, re.MULTILINE)
    keywords.update(code_langs)
    
    # ë§í¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    link_texts = re.findall(r'\[([^\]]+)\]\([^)]+\)', content)
    for lt in link_texts:
        if len(lt) > 2 and len(lt) < 50:
            keywords.add(lt.strip())
    
    # ë¶ˆìš©ì–´ ì œê±° (ê°„ë‹¨í•œ ëª©ë¡)
    stopwords = {
        'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',
        'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
        'could', 'should', 'may', 'might', 'can', 'this', 'that',
        'these', 'those', 'it', 'its', 'and', 'or', 'but', 'if',
        'then', 'else', 'when', 'where', 'how', 'what', 'why',
        'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ë˜ëŠ”', 'ì˜', 'ë¥¼', 'ì„',
    }
    
    filtered = [kw for kw in keywords if kw.lower() not in stopwords]
    
    # ê¸¸ì´ ê¸°ì¤€ ì •ë ¬ í›„ ìƒìœ„ Nê°œ ë°˜í™˜
    sorted_kw = sorted(filtered, key=lambda x: len(x), reverse=True)
    
    return sorted_kw[:max_keywords]


def detect_content_type(content: str) -> str:
    """ì½˜í…ì¸  íƒ€ì… ê°ì§€"""
    # ì½”ë“œ ë¸”ë¡ ë¹„ìœ¨
    code_blocks = re.findall(r'```[\s\S]*?```', content)
    code_length = sum(len(cb) for cb in code_blocks)
    code_ratio = code_length / len(content) if content else 0
    
    if code_ratio > 0.5:
        return "code"
    elif code_ratio > 0.2:
        return "tutorial"
    
    # ë¦¬ìŠ¤íŠ¸ ë¹„ìœ¨
    list_items = re.findall(r'^[-*]\s+', content, re.MULTILINE)
    numbered_items = re.findall(r'^\d+\.\s+', content, re.MULTILINE)
    list_count = len(list_items) + len(numbered_items)
    
    lines = content.count('\n') + 1
    if list_count > lines * 0.3:
        return "list"
    
    # í—¤ë”© ìˆ˜
    headings = re.findall(r'^#{1,6}\s+', content, re.MULTILINE)
    if len(headings) > 5:
        return "documentation"
    
    return "article"


def infer_domain(content: str, keywords: list[str]) -> tuple[str, str]:
    """
    ë„ë©”ì¸ ë° ì„œë¸Œë„ë©”ì¸ ì¶”ë¡ 
    
    Returns:
        (domain, sub_domain) íŠœí”Œ
    """
    content_lower = content.lower()
    keywords_lower = [kw.lower() for kw in keywords]
    
    # ë„ë©”ì¸ í‚¤ì›Œë“œ ë§¤í•‘
    domain_keywords = {
        "programming": [
            "python", "javascript", "typescript", "java", "rust", "go",
            "code", "function", "class", "api", "library", "framework",
            "ì½”ë“œ", "í•¨ìˆ˜", "í”„ë¡œê·¸ë˜ë°", "ê°œë°œ",
        ],
        "machine-learning": [
            "model", "training", "neural", "deep learning", "ai", "ml",
            "embedding", "transformer", "bert", "gpt", "llm",
            "ëª¨ë¸", "í•™ìŠµ", "ì¸ê³µì§€ëŠ¥", "ë”¥ëŸ¬ë‹", "ì„ë² ë”©",
        ],
        "data-science": [
            "data", "pandas", "numpy", "analysis", "visualization",
            "ë°ì´í„°", "ë¶„ì„", "ì‹œê°í™”", "í†µê³„",
        ],
        "devops": [
            "docker", "kubernetes", "ci/cd", "deploy", "container",
            "aws", "azure", "gcp", "cloud",
            "ë°°í¬", "ì»¨í…Œì´ë„ˆ", "í´ë¼ìš°ë“œ",
        ],
        "web": [
            "html", "css", "react", "vue", "angular", "frontend", "backend",
            "ì›¹", "í”„ë¡ íŠ¸ì—”ë“œ", "ë°±ì—”ë“œ",
        ],
        "database": [
            "sql", "nosql", "postgresql", "mongodb", "redis",
            "ë°ì´í„°ë² ì´ìŠ¤", "ì¿¼ë¦¬",
        ],
    }
    
    domain_scores: dict[str, int] = {}
    
    for domain, domain_kws in domain_keywords.items():
        score = 0
        for kw in domain_kws:
            if kw in content_lower:
                score += content_lower.count(kw)
            if kw in keywords_lower:
                score += 5  # í‚¤ì›Œë“œì— ìˆìœ¼ë©´ ê°€ì¤‘ì¹˜
        if score > 0:
            domain_scores[domain] = score
    
    if not domain_scores:
        return "general", ""
    
    # ìµœê³  ì ìˆ˜ ë„ë©”ì¸
    top_domain = max(domain_scores, key=lambda x: domain_scores[x])
    
    # ì„œë¸Œë„ë©”ì¸ì€ í•´ë‹¹ ë„ë©”ì¸ì˜ í‚¤ì›Œë“œ ì¤‘ ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ê²ƒ
    sub_domain = ""
    if top_domain in domain_keywords:
        sub_counts = {}
        for kw in domain_keywords[top_domain]:
            count = content_lower.count(kw)
            if count > 0:
                sub_counts[kw] = count
        if sub_counts:
            sub_domain = max(sub_counts, key=lambda x: sub_counts[x])
    
    return top_domain, sub_domain


def create_summary(content: str, max_length: int = 300) -> str:
    """ì²« ë²ˆì§¸ ë¬¸ë‹¨ì„ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©"""
    # YAML front matter ì œê±°
    if content.startswith("---"):
        parts = content.split("---", 2)
        if len(parts) >= 3:
            content = parts[2]
    
    # í—¤ë”© ì œê±°
    lines = content.strip().split('\n')
    paragraphs = []
    current = []
    
    for line in lines:
        stripped = line.strip()
        if stripped.startswith('#'):
            if current:
                paragraphs.append(' '.join(current))
                current = []
            continue
        if stripped == '':
            if current:
                paragraphs.append(' '.join(current))
                current = []
        else:
            current.append(stripped)
    
    if current:
        paragraphs.append(' '.join(current))
    
    # ì²« ë²ˆì§¸ ì˜ë¯¸ìˆëŠ” ë¬¸ë‹¨
    for p in paragraphs:
        # ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• ì œê±°
        clean = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', p)  # ë§í¬
        clean = re.sub(r'\*\*([^*]+)\*\*', r'\1', clean)  # ë³¼ë“œ
        clean = re.sub(r'\*([^*]+)\*', r'\1', clean)  # ì´íƒ¤ë¦­
        clean = re.sub(r'`([^`]+)`', r'\1', clean)  # ì¸ë¼ì¸ ì½”ë“œ
        
        if len(clean) > 30:
            if len(clean) > max_length:
                return clean[:max_length] + "..."
            return clean
    
    return ""


def prepare_document(
    input_path: Path,
    output_path: Optional[Path] = None,
) -> Path:
    """
    ë¬¸ì„œì— YAML front matter ë©”íƒ€ë°ì´í„° ì¶”ê°€

    Args:
        input_path: ì…ë ¥ íŒŒì¼ ê²½ë¡œ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
        output_path: ì¶œë ¥ ê²½ë¡œ (ê¸°ë³¸ê°’: OUTPUT_DIR)

    Returns:
        ìƒì„±ëœ íŒŒì¼ ê²½ë¡œ
    """
    # íŒŒì¼ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
    content, source_format = convert_to_markdown(input_path)

    # ê¸°ì¡´ front matterê°€ ìˆìœ¼ë©´ ì œê±°
    original_content = content
    if content.startswith("---"):
        parts = content.split("---", 2)
        if len(parts) >= 3:
            original_content = parts[2].strip()

    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    title = extract_title(original_content)
    keywords = extract_keywords(original_content)
    language = detect_language(original_content)
    content_type = detect_content_type(original_content)
    domain, sub_domain = infer_domain(original_content, keywords)
    summary = create_summary(original_content)

    # YAML front matter ìƒì„±
    metadata = {
        "title": title,
        "domain": domain,
        "sub_domain": sub_domain,
        "keywords": keywords,
        "summary": summary,
        "language": language,
        "content_type": content_type,
        "source_file": input_path.name,
        "source_format": source_format,
        "prepared_at": datetime.now().isoformat(),
    }

    # ì¶œë ¥ ê²½ë¡œ ê²°ì • (í•­ìƒ .md í™•ì¥ì)
    if output_path is None:
        output_path = OUTPUT_DIR / input_path.with_suffix('.md').name

    output_path.parent.mkdir(parents=True, exist_ok=True)

    # ìƒˆ ì½˜í…ì¸  ì‘ì„±
    yaml_header = yaml.dump(metadata, allow_unicode=True, sort_keys=False, default_flow_style=False)
    new_content = f"---\n{yaml_header}---\n\n{original_content}"

    output_path.write_text(new_content, encoding="utf-8")

    return output_path


def process_all_documents(
    input_dir: Path = INPUT_DIR,
    output_dir: Path = OUTPUT_DIR,
) -> list[Path]:
    """
    ëª¨ë“  ì…ë ¥ ë¬¸ì„œ ì²˜ë¦¬ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)

    Args:
        input_dir: ì…ë ¥ ë””ë ‰í„°ë¦¬
        output_dir: ì¶œë ¥ ë””ë ‰í„°ë¦¬

    Returns:
        ìƒì„±ëœ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    input_dir = Path(input_dir)
    output_dir = Path(output_dir)

    if not input_dir.exists():
        print(f"âš ï¸ ì…ë ¥ ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {input_dir}")
        return []

    output_dir.mkdir(parents=True, exist_ok=True)

    # ì§€ì›ë˜ëŠ” ëª¨ë“  íŒŒì¼ ìˆ˜ì§‘
    all_files = [
        f for f in input_dir.iterdir()
        if f.is_file() and is_supported_file(f)
    ]

    if not all_files:
        print(f"âš ï¸ ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {input_dir}")
        print(f"   ì§€ì› í˜•ì‹: {', '.join(sorted(SUPPORTED_EXTENSIONS))}")
        return []

    # íŒŒì¼ í˜•ì‹ë³„ í†µê³„
    format_counts: dict[str, int] = {}
    for f in all_files:
        ext = f.suffix.lower()
        format_counts[ext] = format_counts.get(ext, 0) + 1

    print(f"\nğŸ“š ì²˜ë¦¬í•  ë¬¸ì„œ: {len(all_files)}ê°œ")
    print(f"   í˜•ì‹ë³„: {', '.join(f'{ext}({cnt})' for ext, cnt in sorted(format_counts.items()))}")
    print("=" * 50)

    results = []
    for i, file_path in enumerate(all_files, 1):
        print(f"\n[{i}/{len(all_files)}] {file_path.name}")

        try:
            output_path = output_dir / file_path.with_suffix('.md').name
            result = prepare_document(file_path, output_path)

            # ê²°ê³¼ í™•ì¸
            content = result.read_text(encoding="utf-8")
            if content.startswith("---"):
                parts = content.split("---", 2)
                if len(parts) >= 3:
                    meta = yaml.safe_load(parts[1])
                    print(f"   â€¢ ì œëª©: {meta.get('title', 'N/A')[:40]}...")
                    print(f"   â€¢ ë„ë©”ì¸: {meta.get('domain', 'N/A')}")
                    print(f"   â€¢ ì›ë³¸ í˜•ì‹: {meta.get('source_format', 'N/A')}")
                    print(f"   â€¢ í‚¤ì›Œë“œ: {', '.join(meta.get('keywords', [])[:5])}")
                    print(f"   âœ… ì €ì¥: {result.name}")

            results.append(result)

        except UnsupportedFormatException as e:
            print(f"   âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {e}")
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜: {e}")

    print("\n" + "=" * 50)
    print(f"âœ… ì™„ë£Œ: {len(results)}/{len(all_files)} ë¬¸ì„œ ì²˜ë¦¬ë¨")
    print(f"ğŸ“ ì¶œë ¥ ìœ„ì¹˜: {output_dir}")

    return results


def main() -> int:
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="ì…ë ¥ ë¬¸ì„œì— YAML front matter ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."
    )
    parser.add_argument(
        "--input-dir",
        type=Path,
        default=INPUT_DIR,
        help=f"ì…ë ¥ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸ê°’: {INPUT_DIR})",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=OUTPUT_DIR,
        help=f"ì¶œë ¥ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸ê°’: {OUTPUT_DIR})",
    )
    
    args = parser.parse_args()
    
    results = process_all_documents(
        input_dir=args.input_dir,
        output_dir=args.output_dir,
    )
    
    return 0 if results else 1


if __name__ == "__main__":
    exit(main())
